{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Neural Networks: Design Space.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1JmNUcu3auYlaWZQ2AH27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefPony/cs224w-ml-with-graphs/blob/main/Graph_Neural_Networks_Design_Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tZ_uWPluKeJ",
        "outputId": "badaba0a-b629-4960-d291-b6b8f119c438"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p983o-Uf3goM",
        "outputId": "5dc8fc93-ab4b-471e-f1eb-7b5bb62e1911"
      },
      "source": [
        "# Install torch geometric\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=f4295092650b886c4d58ea4edf5ffc4e3932e164154cffde5bf94f5f59d12666\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.62.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=7baba61b3b70fd569f5ceabcf278086989364d28db4f7dad29e47c089086ae62\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.2 outdated-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh9CqxAr3rMf",
        "outputId": "75fd7062-b2f4-4875-fcdd-7050ebebc705"
      },
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  # Load the dataset and transform it to sparse tensor\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "  # Extract the graph\n",
        "  data = dataset[0]\n",
        "  print(data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:26<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 5645.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 562.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiP3NGiD38Z7",
        "outputId": "55edd006-7c7e-4756-a838-3162cfcc595f"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyVZ_4lw4MxH",
        "outputId": "258004d1-726f-4753-89cc-2810604c296a"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  data = dataset[0]\n",
        "\n",
        "  # Make the adjacency matrix to symmetric\n",
        "  data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  # If you use GPU, the device should be cuda\n",
        "  print('Device: {}'.format(device))\n",
        "\n",
        "  data = data.to(device)\n",
        "  split_idx = dataset.get_idx_split()\n",
        "  train_idx = split_idx['train'].to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBQZRx5t8KS3"
      },
      "source": [
        "def train(model, data, loss_fn, optimizer, train_idx):\n",
        "    \n",
        "  model.train()\n",
        "  loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  y_pred = model(data.x, data.adj_t)\n",
        "  loss = loss_fn(y_pred[train_idx], data.y[train_idx].squeeze())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z-SUCMS83SU"
      },
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  y_pred = model(data.x, data.adj_t)\n",
        "  y_pred = y_pred.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "  train_acc = evaluator.eval({\n",
        "      \"y_pred\": y_pred[split_idx[\"train\"]],\n",
        "      \"y_true\": data.y[split_idx[\"train\"]]\n",
        "  })[\"acc\"]\n",
        "  val_acc = evaluator.eval({\n",
        "    \"y_pred\": y_pred[split_idx[\"valid\"]],\n",
        "    \"y_true\": data.y[split_idx[\"valid\"]]\n",
        "  })[\"acc\"]\n",
        "  test_acc = evaluator.eval({\n",
        "    \"y_pred\": y_pred[split_idx[\"test\"]],\n",
        "    \"y_true\": data.y[split_idx[\"test\"]]\n",
        "  })[\"acc\"]\n",
        "\n",
        "  return train_acc, val_acc, test_acc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFgcuHT5-9Br"
      },
      "source": [
        "def train_loop(model, data, optimizer, loss_fn, epochs, split_idx, evaluator):\n",
        "\n",
        "  best_val_acc = 0.\n",
        "\n",
        "  model.reset_parameters()\n",
        "\n",
        "  for e in range(1, epochs+1):\n",
        "    loss = train(model, data, loss_fn, optimizer, split_idx[\"train\"])\n",
        "    train_acc, val_acc, test_acc = test(model, data, split_idx, evaluator)\n",
        "\n",
        "    if val_acc>best_val_acc:\n",
        "      best_model = copy.deepcopy(model)\n",
        "      best_val_acc = val_acc\n",
        "\n",
        "    print(f\"Epoch: {e:02d}, Loss: {loss:.3f},  \"\n",
        "        f\"Train: {100 * train_acc:.3f}%,  \"   \n",
        "        f\"Valid: {100 * val_acc:.3f}%,  \" \n",
        "        f\"Test: {100 * test_acc:.3f}%,  \"\n",
        "        )\n",
        "  return best_model, best_val_acc\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckQ_JxV6tv94"
      },
      "source": [
        "def print_model_accuracy(best_model, data, split_idx, evaluator):\n",
        "  best_result = test(best_model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzlvSjiuNFZ"
      },
      "source": [
        "dataset_evaluator = Evaluator(name=dataset_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Dwgasx-Np2"
      },
      "source": [
        "args = {\n",
        "    \"input_dim\": data.num_features,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"output_dim\": dataset.num_classes,\n",
        "    \"num_layers\": 3,\n",
        "    \"heads\": 2,\n",
        "    \"dropout\": 0.5,\n",
        "    \"epochs\": 100,\n",
        "    \"lr\": 0.01\n",
        "}\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW40CRXupLuR"
      },
      "source": [
        "## Vanilla GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92LyQhFB6Qrf"
      },
      "source": [
        "class GCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
        "      \n",
        "      super(GCN, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [GCNConv(input_dim, hidden_dim)] +\n",
        "          [GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)]+\n",
        "          [GCNConv(hidden_dim, output_dim)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFyB_q-rqAE5",
        "outputId": "60dffbd6-3d5a-4262-c5e2-978495e6789f"
      },
      "source": [
        "gcn = GCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "          output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "          dropout=args[\"dropout\"])\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(gcn.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_gcn, gcn_best_acc = train_loop(gcn.to(device), \n",
        "                                    data, optimizer, nll_loss, \n",
        "                                    args[\"epochs\"], split_idx, \n",
        "                                    dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.186,  Train: 20.815%,  Valid: 26.602%,  Test: 24.025%,  \n",
            "Epoch: 02, Loss: 2.463,  Train: 28.852%,  Valid: 30.353%,  Test: 34.627%,  \n",
            "Epoch: 03, Loss: 2.006,  Train: 25.818%,  Valid: 24.299%,  Test: 29.329%,  \n",
            "Epoch: 04, Loss: 1.804,  Train: 37.233%,  Valid: 39.548%,  Test: 44.767%,  \n",
            "Epoch: 05, Loss: 1.688,  Train: 37.875%,  Valid: 39.840%,  Test: 38.985%,  \n",
            "Epoch: 06, Loss: 1.603,  Train: 27.291%,  Valid: 28.279%,  Test: 28.025%,  \n",
            "Epoch: 07, Loss: 1.539,  Train: 20.201%,  Valid: 16.001%,  Test: 15.162%,  \n",
            "Epoch: 08, Loss: 1.484,  Train: 20.213%,  Valid: 14.923%,  Test: 13.164%,  \n",
            "Epoch: 09, Loss: 1.428,  Train: 23.038%,  Valid: 19.957%,  Test: 21.795%,  \n",
            "Epoch: 10, Loss: 1.379,  Train: 27.408%,  Valid: 25.390%,  Test: 28.914%,  \n",
            "Epoch: 11, Loss: 1.353,  Train: 30.664%,  Valid: 28.722%,  Test: 33.148%,  \n",
            "Epoch: 12, Loss: 1.325,  Train: 33.128%,  Valid: 32.387%,  Test: 36.850%,  \n",
            "Epoch: 13, Loss: 1.297,  Train: 36.166%,  Valid: 36.263%,  Test: 41.364%,  \n",
            "Epoch: 14, Loss: 1.276,  Train: 39.452%,  Valid: 40.810%,  Test: 44.952%,  \n",
            "Epoch: 15, Loss: 1.258,  Train: 42.363%,  Valid: 45.458%,  Test: 49.295%,  \n",
            "Epoch: 16, Loss: 1.240,  Train: 44.308%,  Valid: 48.005%,  Test: 51.563%,  \n",
            "Epoch: 17, Loss: 1.227,  Train: 46.970%,  Valid: 50.525%,  Test: 53.700%,  \n",
            "Epoch: 18, Loss: 1.216,  Train: 48.561%,  Valid: 51.911%,  Test: 54.844%,  \n",
            "Epoch: 19, Loss: 1.201,  Train: 50.452%,  Valid: 53.415%,  Test: 56.301%,  \n",
            "Epoch: 20, Loss: 1.187,  Train: 52.574%,  Valid: 55.264%,  Test: 58.052%,  \n",
            "Epoch: 21, Loss: 1.174,  Train: 54.379%,  Valid: 56.928%,  Test: 59.208%,  \n",
            "Epoch: 22, Loss: 1.172,  Train: 56.534%,  Valid: 58.589%,  Test: 60.426%,  \n",
            "Epoch: 23, Loss: 1.159,  Train: 58.013%,  Valid: 59.566%,  Test: 61.476%,  \n",
            "Epoch: 24, Loss: 1.152,  Train: 59.213%,  Valid: 60.505%,  Test: 61.971%,  \n",
            "Epoch: 25, Loss: 1.146,  Train: 59.942%,  Valid: 61.193%,  Test: 62.591%,  \n",
            "Epoch: 26, Loss: 1.135,  Train: 60.173%,  Valid: 61.482%,  Test: 63.031%,  \n",
            "Epoch: 27, Loss: 1.127,  Train: 59.872%,  Valid: 60.650%,  Test: 62.428%,  \n",
            "Epoch: 28, Loss: 1.117,  Train: 60.685%,  Valid: 61.220%,  Test: 62.813%,  \n",
            "Epoch: 29, Loss: 1.114,  Train: 61.027%,  Valid: 61.348%,  Test: 62.945%,  \n",
            "Epoch: 30, Loss: 1.105,  Train: 61.942%,  Valid: 62.230%,  Test: 63.537%,  \n",
            "Epoch: 31, Loss: 1.097,  Train: 63.094%,  Valid: 63.187%,  Test: 64.113%,  \n",
            "Epoch: 32, Loss: 1.090,  Train: 64.419%,  Valid: 64.603%,  Test: 65.152%,  \n",
            "Epoch: 33, Loss: 1.084,  Train: 64.709%,  Valid: 65.646%,  Test: 65.654%,  \n",
            "Epoch: 34, Loss: 1.077,  Train: 64.742%,  Valid: 64.938%,  Test: 65.774%,  \n",
            "Epoch: 35, Loss: 1.074,  Train: 64.283%,  Valid: 64.673%,  Test: 65.772%,  \n",
            "Epoch: 36, Loss: 1.072,  Train: 64.614%,  Valid: 64.499%,  Test: 65.763%,  \n",
            "Epoch: 37, Loss: 1.065,  Train: 64.956%,  Valid: 65.204%,  Test: 66.066%,  \n",
            "Epoch: 38, Loss: 1.062,  Train: 65.749%,  Valid: 65.391%,  Test: 66.393%,  \n",
            "Epoch: 39, Loss: 1.056,  Train: 66.136%,  Valid: 65.918%,  Test: 66.753%,  \n",
            "Epoch: 40, Loss: 1.051,  Train: 66.727%,  Valid: 66.569%,  Test: 67.348%,  \n",
            "Epoch: 41, Loss: 1.052,  Train: 67.103%,  Valid: 67.546%,  Test: 67.790%,  \n",
            "Epoch: 42, Loss: 1.046,  Train: 67.341%,  Valid: 67.569%,  Test: 68.043%,  \n",
            "Epoch: 43, Loss: 1.043,  Train: 67.557%,  Valid: 68.076%,  Test: 68.199%,  \n",
            "Epoch: 44, Loss: 1.038,  Train: 67.908%,  Valid: 68.251%,  Test: 68.601%,  \n",
            "Epoch: 45, Loss: 1.033,  Train: 67.980%,  Valid: 68.053%,  Test: 68.389%,  \n",
            "Epoch: 46, Loss: 1.031,  Train: 68.422%,  Valid: 68.620%,  Test: 68.494%,  \n",
            "Epoch: 47, Loss: 1.028,  Train: 68.692%,  Valid: 68.613%,  Test: 68.467%,  \n",
            "Epoch: 48, Loss: 1.023,  Train: 68.697%,  Valid: 68.955%,  Test: 68.774%,  \n",
            "Epoch: 49, Loss: 1.020,  Train: 68.819%,  Valid: 68.804%,  Test: 68.677%,  \n",
            "Epoch: 50, Loss: 1.017,  Train: 68.812%,  Valid: 68.922%,  Test: 68.809%,  \n",
            "Epoch: 51, Loss: 1.015,  Train: 69.017%,  Valid: 68.670%,  Test: 68.580%,  \n",
            "Epoch: 52, Loss: 1.011,  Train: 69.132%,  Valid: 68.975%,  Test: 68.823%,  \n",
            "Epoch: 53, Loss: 1.009,  Train: 69.337%,  Valid: 69.190%,  Test: 68.763%,  \n",
            "Epoch: 54, Loss: 1.006,  Train: 69.438%,  Valid: 68.989%,  Test: 68.913%,  \n",
            "Epoch: 55, Loss: 1.005,  Train: 69.555%,  Valid: 68.925%,  Test: 68.681%,  \n",
            "Epoch: 56, Loss: 1.000,  Train: 69.731%,  Valid: 68.855%,  Test: 68.516%,  \n",
            "Epoch: 57, Loss: 0.996,  Train: 69.676%,  Valid: 68.969%,  Test: 68.455%,  \n",
            "Epoch: 58, Loss: 0.998,  Train: 69.681%,  Valid: 69.096%,  Test: 68.625%,  \n",
            "Epoch: 59, Loss: 0.992,  Train: 69.868%,  Valid: 69.298%,  Test: 69.082%,  \n",
            "Epoch: 60, Loss: 0.989,  Train: 69.767%,  Valid: 69.499%,  Test: 69.148%,  \n",
            "Epoch: 61, Loss: 0.988,  Train: 70.034%,  Valid: 69.647%,  Test: 69.222%,  \n",
            "Epoch: 62, Loss: 0.982,  Train: 70.112%,  Valid: 69.580%,  Test: 69.315%,  \n",
            "Epoch: 63, Loss: 0.983,  Train: 70.137%,  Valid: 69.798%,  Test: 69.292%,  \n",
            "Epoch: 64, Loss: 0.979,  Train: 70.220%,  Valid: 69.606%,  Test: 69.123%,  \n",
            "Epoch: 65, Loss: 0.977,  Train: 70.259%,  Valid: 69.529%,  Test: 68.549%,  \n",
            "Epoch: 66, Loss: 0.978,  Train: 70.392%,  Valid: 69.677%,  Test: 68.817%,  \n",
            "Epoch: 67, Loss: 0.974,  Train: 70.363%,  Valid: 69.556%,  Test: 69.187%,  \n",
            "Epoch: 68, Loss: 0.972,  Train: 70.464%,  Valid: 69.764%,  Test: 69.393%,  \n",
            "Epoch: 69, Loss: 0.970,  Train: 70.555%,  Valid: 69.962%,  Test: 68.957%,  \n",
            "Epoch: 70, Loss: 0.968,  Train: 70.412%,  Valid: 70.080%,  Test: 69.134%,  \n",
            "Epoch: 71, Loss: 0.965,  Train: 70.485%,  Valid: 69.791%,  Test: 69.383%,  \n",
            "Epoch: 72, Loss: 0.965,  Train: 70.549%,  Valid: 69.935%,  Test: 69.701%,  \n",
            "Epoch: 73, Loss: 0.963,  Train: 70.569%,  Valid: 70.194%,  Test: 69.765%,  \n",
            "Epoch: 74, Loss: 0.961,  Train: 70.831%,  Valid: 69.912%,  Test: 69.453%,  \n",
            "Epoch: 75, Loss: 0.959,  Train: 70.738%,  Valid: 69.969%,  Test: 68.716%,  \n",
            "Epoch: 76, Loss: 0.955,  Train: 70.770%,  Valid: 69.724%,  Test: 68.590%,  \n",
            "Epoch: 77, Loss: 0.957,  Train: 70.940%,  Valid: 69.720%,  Test: 68.946%,  \n",
            "Epoch: 78, Loss: 0.953,  Train: 71.096%,  Valid: 70.026%,  Test: 69.399%,  \n",
            "Epoch: 79, Loss: 0.952,  Train: 70.994%,  Valid: 69.929%,  Test: 69.049%,  \n",
            "Epoch: 80, Loss: 0.952,  Train: 71.179%,  Valid: 69.959%,  Test: 68.660%,  \n",
            "Epoch: 81, Loss: 0.949,  Train: 71.156%,  Valid: 70.046%,  Test: 68.889%,  \n",
            "Epoch: 82, Loss: 0.947,  Train: 71.066%,  Valid: 70.301%,  Test: 69.380%,  \n",
            "Epoch: 83, Loss: 0.947,  Train: 71.217%,  Valid: 70.328%,  Test: 69.876%,  \n",
            "Epoch: 84, Loss: 0.943,  Train: 71.293%,  Valid: 70.194%,  Test: 69.253%,  \n",
            "Epoch: 85, Loss: 0.941,  Train: 71.168%,  Valid: 69.835%,  Test: 68.436%,  \n",
            "Epoch: 86, Loss: 0.939,  Train: 71.219%,  Valid: 70.002%,  Test: 68.247%,  \n",
            "Epoch: 87, Loss: 0.937,  Train: 71.337%,  Valid: 69.976%,  Test: 68.983%,  \n",
            "Epoch: 88, Loss: 0.938,  Train: 71.322%,  Valid: 70.261%,  Test: 69.500%,  \n",
            "Epoch: 89, Loss: 0.934,  Train: 71.480%,  Valid: 70.160%,  Test: 69.378%,  \n",
            "Epoch: 90, Loss: 0.934,  Train: 71.480%,  Valid: 70.046%,  Test: 69.368%,  \n",
            "Epoch: 91, Loss: 0.934,  Train: 71.480%,  Valid: 70.210%,  Test: 69.189%,  \n",
            "Epoch: 92, Loss: 0.934,  Train: 71.565%,  Valid: 70.204%,  Test: 69.467%,  \n",
            "Epoch: 93, Loss: 0.929,  Train: 71.487%,  Valid: 70.418%,  Test: 69.629%,  \n",
            "Epoch: 94, Loss: 0.928,  Train: 71.621%,  Valid: 70.405%,  Test: 69.471%,  \n",
            "Epoch: 95, Loss: 0.928,  Train: 71.566%,  Valid: 69.965%,  Test: 69.103%,  \n",
            "Epoch: 96, Loss: 0.922,  Train: 71.594%,  Valid: 70.358%,  Test: 69.527%,  \n",
            "Epoch: 97, Loss: 0.922,  Train: 71.809%,  Valid: 70.308%,  Test: 69.453%,  \n",
            "Epoch: 98, Loss: 0.920,  Train: 71.665%,  Valid: 70.519%,  Test: 69.656%,  \n",
            "Epoch: 99, Loss: 0.921,  Train: 71.671%,  Valid: 70.308%,  Test: 68.992%,  \n",
            "Epoch: 100, Loss: 0.920,  Train: 71.616%,  Valid: 69.945%,  Test: 69.027%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7qxVZVdyYSX",
        "outputId": "b3ef633f-441d-4973-8010-af5849f5df5d"
      },
      "source": [
        "print_model_accuracy(best_gcn, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.67%, Valid: 70.27% Test: 69.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49hK34vUpSyN"
      },
      "source": [
        "## GraphSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKsTeokbCcE"
      },
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout,\n",
        "                 normalize=False):\n",
        "      \n",
        "      super(SAGE, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [SAGEConv(input_dim, hidden_dim, normalize=normalize)] +\n",
        "          [SAGEConv(hidden_dim, hidden_dim, normalize=normalize) \n",
        "           for _ in range(num_layers - 2)]+\n",
        "          [SAGEConv(hidden_dim, output_dim, normalize=normalize)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLAwUs5vwrj3"
      },
      "source": [
        "### No normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIH9szzcuV7e",
        "outputId": "abbff3f9-624c-4618-a75a-0cd04801a4b3"
      },
      "source": [
        "sage = SAGE(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "            output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "            dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(sage.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sage, sage_best_acc = train_loop(sage.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.943,  Train: 31.739%,  Valid: 33.303%,  Test: 32.663%,  \n",
            "Epoch: 02, Loss: 2.582,  Train: 29.385%,  Valid: 33.649%,  Test: 32.541%,  \n",
            "Epoch: 03, Loss: 2.220,  Train: 35.980%,  Valid: 41.589%,  Test: 44.392%,  \n",
            "Epoch: 04, Loss: 2.047,  Train: 40.289%,  Valid: 44.793%,  Test: 46.409%,  \n",
            "Epoch: 05, Loss: 1.825,  Train: 44.133%,  Valid: 46.693%,  Test: 46.773%,  \n",
            "Epoch: 06, Loss: 1.688,  Train: 47.782%,  Valid: 50.203%,  Test: 50.022%,  \n",
            "Epoch: 07, Loss: 1.617,  Train: 49.836%,  Valid: 53.646%,  Test: 54.021%,  \n",
            "Epoch: 08, Loss: 1.539,  Train: 50.304%,  Valid: 54.066%,  Test: 54.256%,  \n",
            "Epoch: 09, Loss: 1.485,  Train: 51.090%,  Valid: 54.562%,  Test: 54.404%,  \n",
            "Epoch: 10, Loss: 1.448,  Train: 52.172%,  Valid: 54.938%,  Test: 54.799%,  \n",
            "Epoch: 11, Loss: 1.416,  Train: 53.626%,  Valid: 56.767%,  Test: 56.745%,  \n",
            "Epoch: 12, Loss: 1.384,  Train: 55.081%,  Valid: 58.190%,  Test: 58.361%,  \n",
            "Epoch: 13, Loss: 1.361,  Train: 56.581%,  Valid: 59.713%,  Test: 59.982%,  \n",
            "Epoch: 14, Loss: 1.346,  Train: 58.020%,  Valid: 60.740%,  Test: 61.373%,  \n",
            "Epoch: 15, Loss: 1.323,  Train: 58.915%,  Valid: 62.069%,  Test: 62.313%,  \n",
            "Epoch: 16, Loss: 1.300,  Train: 59.590%,  Valid: 62.143%,  Test: 62.556%,  \n",
            "Epoch: 17, Loss: 1.276,  Train: 60.202%,  Valid: 62.549%,  Test: 62.572%,  \n",
            "Epoch: 18, Loss: 1.258,  Train: 60.560%,  Valid: 62.831%,  Test: 62.932%,  \n",
            "Epoch: 19, Loss: 1.245,  Train: 61.054%,  Valid: 63.368%,  Test: 63.912%,  \n",
            "Epoch: 20, Loss: 1.232,  Train: 61.410%,  Valid: 64.049%,  Test: 64.381%,  \n",
            "Epoch: 21, Loss: 1.219,  Train: 62.468%,  Valid: 64.626%,  Test: 64.967%,  \n",
            "Epoch: 22, Loss: 1.209,  Train: 62.945%,  Valid: 64.942%,  Test: 65.317%,  \n",
            "Epoch: 23, Loss: 1.200,  Train: 63.550%,  Valid: 65.422%,  Test: 65.383%,  \n",
            "Epoch: 24, Loss: 1.191,  Train: 63.883%,  Valid: 65.502%,  Test: 65.552%,  \n",
            "Epoch: 25, Loss: 1.175,  Train: 64.255%,  Valid: 65.855%,  Test: 65.445%,  \n",
            "Epoch: 26, Loss: 1.162,  Train: 64.452%,  Valid: 65.771%,  Test: 65.370%,  \n",
            "Epoch: 27, Loss: 1.154,  Train: 64.675%,  Valid: 65.979%,  Test: 65.724%,  \n",
            "Epoch: 28, Loss: 1.147,  Train: 65.250%,  Valid: 66.207%,  Test: 66.132%,  \n",
            "Epoch: 29, Loss: 1.136,  Train: 65.650%,  Valid: 66.489%,  Test: 66.132%,  \n",
            "Epoch: 30, Loss: 1.132,  Train: 66.034%,  Valid: 66.761%,  Test: 66.187%,  \n",
            "Epoch: 31, Loss: 1.124,  Train: 66.202%,  Valid: 66.845%,  Test: 66.058%,  \n",
            "Epoch: 32, Loss: 1.116,  Train: 66.342%,  Valid: 66.777%,  Test: 66.056%,  \n",
            "Epoch: 33, Loss: 1.111,  Train: 66.473%,  Valid: 67.408%,  Test: 66.541%,  \n",
            "Epoch: 34, Loss: 1.102,  Train: 66.839%,  Valid: 67.539%,  Test: 67.064%,  \n",
            "Epoch: 35, Loss: 1.095,  Train: 67.032%,  Valid: 67.724%,  Test: 67.216%,  \n",
            "Epoch: 36, Loss: 1.090,  Train: 67.157%,  Valid: 67.905%,  Test: 67.422%,  \n",
            "Epoch: 37, Loss: 1.085,  Train: 67.511%,  Valid: 67.945%,  Test: 67.590%,  \n",
            "Epoch: 38, Loss: 1.081,  Train: 67.718%,  Valid: 68.160%,  Test: 67.321%,  \n",
            "Epoch: 39, Loss: 1.072,  Train: 67.700%,  Valid: 68.116%,  Test: 67.446%,  \n",
            "Epoch: 40, Loss: 1.069,  Train: 67.911%,  Valid: 68.274%,  Test: 67.630%,  \n",
            "Epoch: 41, Loss: 1.063,  Train: 67.902%,  Valid: 68.073%,  Test: 67.720%,  \n",
            "Epoch: 42, Loss: 1.059,  Train: 68.116%,  Valid: 68.234%,  Test: 67.971%,  \n",
            "Epoch: 43, Loss: 1.053,  Train: 68.239%,  Valid: 68.381%,  Test: 68.125%,  \n",
            "Epoch: 44, Loss: 1.050,  Train: 68.215%,  Valid: 68.697%,  Test: 68.370%,  \n",
            "Epoch: 45, Loss: 1.049,  Train: 68.451%,  Valid: 68.727%,  Test: 68.272%,  \n",
            "Epoch: 46, Loss: 1.041,  Train: 68.569%,  Valid: 68.586%,  Test: 67.839%,  \n",
            "Epoch: 47, Loss: 1.037,  Train: 68.789%,  Valid: 68.912%,  Test: 68.175%,  \n",
            "Epoch: 48, Loss: 1.031,  Train: 68.812%,  Valid: 68.630%,  Test: 68.562%,  \n",
            "Epoch: 49, Loss: 1.029,  Train: 68.977%,  Valid: 68.858%,  Test: 68.658%,  \n",
            "Epoch: 50, Loss: 1.024,  Train: 68.737%,  Valid: 69.006%,  Test: 68.479%,  \n",
            "Epoch: 51, Loss: 1.022,  Train: 68.970%,  Valid: 69.086%,  Test: 68.418%,  \n",
            "Epoch: 52, Loss: 1.017,  Train: 69.100%,  Valid: 68.942%,  Test: 68.294%,  \n",
            "Epoch: 53, Loss: 1.013,  Train: 69.251%,  Valid: 68.918%,  Test: 68.761%,  \n",
            "Epoch: 54, Loss: 1.012,  Train: 69.538%,  Valid: 69.137%,  Test: 68.685%,  \n",
            "Epoch: 55, Loss: 1.007,  Train: 69.155%,  Valid: 68.892%,  Test: 68.313%,  \n",
            "Epoch: 56, Loss: 1.002,  Train: 69.541%,  Valid: 69.016%,  Test: 68.683%,  \n",
            "Epoch: 57, Loss: 1.002,  Train: 69.527%,  Valid: 69.079%,  Test: 68.652%,  \n",
            "Epoch: 58, Loss: 0.998,  Train: 69.430%,  Valid: 68.720%,  Test: 68.498%,  \n",
            "Epoch: 59, Loss: 0.994,  Train: 69.799%,  Valid: 68.928%,  Test: 68.214%,  \n",
            "Epoch: 60, Loss: 0.993,  Train: 69.762%,  Valid: 68.912%,  Test: 68.451%,  \n",
            "Epoch: 61, Loss: 0.990,  Train: 69.855%,  Valid: 69.187%,  Test: 68.551%,  \n",
            "Epoch: 62, Loss: 0.987,  Train: 69.975%,  Valid: 69.002%,  Test: 68.852%,  \n",
            "Epoch: 63, Loss: 0.986,  Train: 69.775%,  Valid: 69.113%,  Test: 68.702%,  \n",
            "Epoch: 64, Loss: 0.978,  Train: 69.882%,  Valid: 69.106%,  Test: 69.062%,  \n",
            "Epoch: 65, Loss: 0.974,  Train: 69.956%,  Valid: 69.217%,  Test: 68.965%,  \n",
            "Epoch: 66, Loss: 0.974,  Train: 70.259%,  Valid: 69.472%,  Test: 68.874%,  \n",
            "Epoch: 67, Loss: 0.973,  Train: 70.339%,  Valid: 69.140%,  Test: 68.584%,  \n",
            "Epoch: 68, Loss: 0.967,  Train: 70.320%,  Valid: 69.291%,  Test: 68.881%,  \n",
            "Epoch: 69, Loss: 0.964,  Train: 69.994%,  Valid: 68.996%,  Test: 69.158%,  \n",
            "Epoch: 70, Loss: 0.965,  Train: 70.070%,  Valid: 68.939%,  Test: 68.920%,  \n",
            "Epoch: 71, Loss: 0.962,  Train: 70.363%,  Valid: 69.405%,  Test: 69.041%,  \n",
            "Epoch: 72, Loss: 0.958,  Train: 70.321%,  Valid: 69.126%,  Test: 68.953%,  \n",
            "Epoch: 73, Loss: 0.954,  Train: 70.326%,  Valid: 69.647%,  Test: 69.230%,  \n",
            "Epoch: 74, Loss: 0.954,  Train: 70.244%,  Valid: 69.204%,  Test: 69.140%,  \n",
            "Epoch: 75, Loss: 0.949,  Train: 70.145%,  Valid: 69.257%,  Test: 69.095%,  \n",
            "Epoch: 76, Loss: 0.947,  Train: 70.286%,  Valid: 69.113%,  Test: 69.109%,  \n",
            "Epoch: 77, Loss: 0.945,  Train: 70.395%,  Valid: 69.046%,  Test: 68.839%,  \n",
            "Epoch: 78, Loss: 0.943,  Train: 70.566%,  Valid: 69.247%,  Test: 69.045%,  \n",
            "Epoch: 79, Loss: 0.941,  Train: 70.505%,  Valid: 69.076%,  Test: 69.041%,  \n",
            "Epoch: 80, Loss: 0.939,  Train: 70.616%,  Valid: 69.217%,  Test: 69.261%,  \n",
            "Epoch: 81, Loss: 0.936,  Train: 70.662%,  Valid: 69.301%,  Test: 69.134%,  \n",
            "Epoch: 82, Loss: 0.935,  Train: 70.743%,  Valid: 69.126%,  Test: 69.064%,  \n",
            "Epoch: 83, Loss: 0.931,  Train: 70.685%,  Valid: 69.116%,  Test: 69.006%,  \n",
            "Epoch: 84, Loss: 0.928,  Train: 70.826%,  Valid: 68.992%,  Test: 69.267%,  \n",
            "Epoch: 85, Loss: 0.926,  Train: 70.756%,  Valid: 69.254%,  Test: 69.123%,  \n",
            "Epoch: 86, Loss: 0.929,  Train: 70.975%,  Valid: 69.583%,  Test: 69.222%,  \n",
            "Epoch: 87, Loss: 0.922,  Train: 70.977%,  Valid: 69.482%,  Test: 69.156%,  \n",
            "Epoch: 88, Loss: 0.920,  Train: 71.014%,  Valid: 69.509%,  Test: 69.175%,  \n",
            "Epoch: 89, Loss: 0.917,  Train: 71.173%,  Valid: 69.368%,  Test: 69.278%,  \n",
            "Epoch: 90, Loss: 0.917,  Train: 71.104%,  Valid: 69.690%,  Test: 69.290%,  \n",
            "Epoch: 91, Loss: 0.917,  Train: 71.203%,  Valid: 69.798%,  Test: 69.259%,  \n",
            "Epoch: 92, Loss: 0.913,  Train: 71.251%,  Valid: 69.630%,  Test: 69.329%,  \n",
            "Epoch: 93, Loss: 0.909,  Train: 71.424%,  Valid: 69.630%,  Test: 69.220%,  \n",
            "Epoch: 94, Loss: 0.908,  Train: 71.188%,  Valid: 69.439%,  Test: 69.020%,  \n",
            "Epoch: 95, Loss: 0.906,  Train: 71.179%,  Valid: 69.267%,  Test: 69.420%,  \n",
            "Epoch: 96, Loss: 0.902,  Train: 71.538%,  Valid: 69.965%,  Test: 69.492%,  \n",
            "Epoch: 97, Loss: 0.904,  Train: 71.648%,  Valid: 69.824%,  Test: 69.405%,  \n",
            "Epoch: 98, Loss: 0.899,  Train: 71.732%,  Valid: 70.143%,  Test: 69.313%,  \n",
            "Epoch: 99, Loss: 0.898,  Train: 71.830%,  Valid: 69.972%,  Test: 69.459%,  \n",
            "Epoch: 100, Loss: 0.897,  Train: 71.703%,  Valid: 69.915%,  Test: 69.592%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxMTkyK2y-Zb",
        "outputId": "863565f5-c089-40b7-b25a-548cd67a08e9"
      },
      "source": [
        "print_model_accuracy(best_sage, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.93%, Valid: 70.39% Test: 69.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksmxQNW_wuyv"
      },
      "source": [
        "### With normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUEmWo_KwfQB",
        "outputId": "fbab740a-307f-4b09-cf19-1cf778bfc168"
      },
      "source": [
        "sagenorm = SAGE(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "            output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "            dropout=args[\"dropout\"], normalize=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(sagenorm.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sagenorm, sagenorm_best_acc = train_loop(sagenorm.to(device), \n",
        "                                              data, optimizer, nll_loss, \n",
        "                                              args[\"epochs\"], split_idx, \n",
        "                                              dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.688,  Train: 24.851%,  Valid: 22.222%,  Test: 25.470%,  \n",
            "Epoch: 02, Loss: 3.432,  Train: 29.644%,  Valid: 32.340%,  Test: 36.185%,  \n",
            "Epoch: 03, Loss: 3.390,  Train: 30.794%,  Valid: 36.109%,  Test: 39.921%,  \n",
            "Epoch: 04, Loss: 3.353,  Train: 30.813%,  Valid: 37.807%,  Test: 41.164%,  \n",
            "Epoch: 05, Loss: 3.315,  Train: 31.129%,  Valid: 38.867%,  Test: 41.777%,  \n",
            "Epoch: 06, Loss: 3.282,  Train: 31.043%,  Valid: 38.521%,  Test: 40.856%,  \n",
            "Epoch: 07, Loss: 3.259,  Train: 30.323%,  Valid: 37.273%,  Test: 38.382%,  \n",
            "Epoch: 08, Loss: 3.243,  Train: 29.462%,  Valid: 35.719%,  Test: 36.385%,  \n",
            "Epoch: 09, Loss: 3.230,  Train: 28.435%,  Valid: 34.800%,  Test: 35.018%,  \n",
            "Epoch: 10, Loss: 3.216,  Train: 28.181%,  Valid: 35.233%,  Test: 35.586%,  \n",
            "Epoch: 11, Loss: 3.201,  Train: 28.875%,  Valid: 36.273%,  Test: 37.269%,  \n",
            "Epoch: 12, Loss: 3.187,  Train: 30.389%,  Valid: 39.001%,  Test: 40.759%,  \n",
            "Epoch: 13, Loss: 3.174,  Train: 32.539%,  Valid: 41.303%,  Test: 43.954%,  \n",
            "Epoch: 14, Loss: 3.161,  Train: 33.860%,  Valid: 42.387%,  Test: 45.865%,  \n",
            "Epoch: 15, Loss: 3.151,  Train: 34.388%,  Valid: 42.236%,  Test: 45.870%,  \n",
            "Epoch: 16, Loss: 3.143,  Train: 34.442%,  Valid: 41.511%,  Test: 45.326%,  \n",
            "Epoch: 17, Loss: 3.136,  Train: 33.794%,  Valid: 39.968%,  Test: 44.150%,  \n",
            "Epoch: 18, Loss: 3.129,  Train: 33.099%,  Valid: 38.548%,  Test: 43.380%,  \n",
            "Epoch: 19, Loss: 3.124,  Train: 33.512%,  Valid: 38.149%,  Test: 42.683%,  \n",
            "Epoch: 20, Loss: 3.119,  Train: 33.554%,  Valid: 38.186%,  Test: 42.518%,  \n",
            "Epoch: 21, Loss: 3.114,  Train: 34.138%,  Valid: 38.337%,  Test: 42.606%,  \n",
            "Epoch: 22, Loss: 3.110,  Train: 35.179%,  Valid: 39.709%,  Test: 43.713%,  \n",
            "Epoch: 23, Loss: 3.106,  Train: 36.291%,  Valid: 40.327%,  Test: 44.450%,  \n",
            "Epoch: 24, Loss: 3.102,  Train: 37.516%,  Valid: 41.679%,  Test: 45.863%,  \n",
            "Epoch: 25, Loss: 3.099,  Train: 39.015%,  Valid: 43.340%,  Test: 47.116%,  \n",
            "Epoch: 26, Loss: 3.095,  Train: 40.565%,  Valid: 45.565%,  Test: 48.664%,  \n",
            "Epoch: 27, Loss: 3.091,  Train: 42.282%,  Valid: 47.283%,  Test: 50.303%,  \n",
            "Epoch: 28, Loss: 3.088,  Train: 43.579%,  Valid: 49.109%,  Test: 51.847%,  \n",
            "Epoch: 29, Loss: 3.085,  Train: 45.167%,  Valid: 50.676%,  Test: 53.305%,  \n",
            "Epoch: 30, Loss: 3.081,  Train: 46.549%,  Valid: 51.767%,  Test: 54.019%,  \n",
            "Epoch: 31, Loss: 3.078,  Train: 47.411%,  Valid: 52.680%,  Test: 55.019%,  \n",
            "Epoch: 32, Loss: 3.075,  Train: 48.608%,  Valid: 53.371%,  Test: 55.239%,  \n",
            "Epoch: 33, Loss: 3.072,  Train: 49.814%,  Valid: 54.066%,  Test: 55.793%,  \n",
            "Epoch: 34, Loss: 3.069,  Train: 50.896%,  Valid: 54.821%,  Test: 56.420%,  \n",
            "Epoch: 35, Loss: 3.067,  Train: 52.150%,  Valid: 55.592%,  Test: 56.704%,  \n",
            "Epoch: 36, Loss: 3.064,  Train: 53.552%,  Valid: 56.572%,  Test: 57.379%,  \n",
            "Epoch: 37, Loss: 3.062,  Train: 54.490%,  Valid: 57.237%,  Test: 57.416%,  \n",
            "Epoch: 38, Loss: 3.060,  Train: 55.638%,  Valid: 57.878%,  Test: 57.961%,  \n",
            "Epoch: 39, Loss: 3.058,  Train: 56.055%,  Valid: 57.911%,  Test: 57.896%,  \n",
            "Epoch: 40, Loss: 3.056,  Train: 56.676%,  Valid: 58.210%,  Test: 57.916%,  \n",
            "Epoch: 41, Loss: 3.054,  Train: 57.199%,  Valid: 58.183%,  Test: 57.867%,  \n",
            "Epoch: 42, Loss: 3.053,  Train: 57.409%,  Valid: 58.425%,  Test: 57.809%,  \n",
            "Epoch: 43, Loss: 3.051,  Train: 57.524%,  Valid: 58.415%,  Test: 57.766%,  \n",
            "Epoch: 44, Loss: 3.049,  Train: 58.103%,  Valid: 58.817%,  Test: 58.081%,  \n",
            "Epoch: 45, Loss: 3.047,  Train: 58.691%,  Valid: 59.515%,  Test: 58.618%,  \n",
            "Epoch: 46, Loss: 3.045,  Train: 59.484%,  Valid: 60.106%,  Test: 59.396%,  \n",
            "Epoch: 47, Loss: 3.044,  Train: 60.219%,  Valid: 60.596%,  Test: 60.077%,  \n",
            "Epoch: 48, Loss: 3.042,  Train: 60.453%,  Valid: 60.985%,  Test: 60.352%,  \n",
            "Epoch: 49, Loss: 3.041,  Train: 60.880%,  Valid: 61.140%,  Test: 60.507%,  \n",
            "Epoch: 50, Loss: 3.039,  Train: 60.980%,  Valid: 61.502%,  Test: 60.589%,  \n",
            "Epoch: 51, Loss: 3.038,  Train: 61.333%,  Valid: 61.314%,  Test: 60.529%,  \n",
            "Epoch: 52, Loss: 3.035,  Train: 61.893%,  Valid: 61.918%,  Test: 61.280%,  \n",
            "Epoch: 53, Loss: 3.035,  Train: 62.501%,  Valid: 62.609%,  Test: 61.745%,  \n",
            "Epoch: 54, Loss: 3.033,  Train: 63.093%,  Valid: 63.277%,  Test: 62.632%,  \n",
            "Epoch: 55, Loss: 3.032,  Train: 63.549%,  Valid: 63.630%,  Test: 63.169%,  \n",
            "Epoch: 56, Loss: 3.031,  Train: 63.761%,  Valid: 63.878%,  Test: 63.570%,  \n",
            "Epoch: 57, Loss: 3.029,  Train: 64.210%,  Valid: 64.220%,  Test: 63.743%,  \n",
            "Epoch: 58, Loss: 3.028,  Train: 64.530%,  Valid: 64.355%,  Test: 63.938%,  \n",
            "Epoch: 59, Loss: 3.026,  Train: 64.767%,  Valid: 64.633%,  Test: 64.356%,  \n",
            "Epoch: 60, Loss: 3.025,  Train: 65.135%,  Valid: 65.395%,  Test: 64.517%,  \n",
            "Epoch: 61, Loss: 3.024,  Train: 65.354%,  Valid: 65.204%,  Test: 64.780%,  \n",
            "Epoch: 62, Loss: 3.022,  Train: 65.514%,  Valid: 65.452%,  Test: 64.885%,  \n",
            "Epoch: 63, Loss: 3.022,  Train: 65.743%,  Valid: 65.536%,  Test: 64.975%,  \n",
            "Epoch: 64, Loss: 3.020,  Train: 66.047%,  Valid: 65.844%,  Test: 65.224%,  \n",
            "Epoch: 65, Loss: 3.019,  Train: 66.416%,  Valid: 66.120%,  Test: 65.366%,  \n",
            "Epoch: 66, Loss: 3.018,  Train: 66.729%,  Valid: 66.455%,  Test: 65.473%,  \n",
            "Epoch: 67, Loss: 3.016,  Train: 66.747%,  Valid: 66.509%,  Test: 65.440%,  \n",
            "Epoch: 68, Loss: 3.016,  Train: 66.884%,  Valid: 66.858%,  Test: 65.603%,  \n",
            "Epoch: 69, Loss: 3.015,  Train: 67.064%,  Valid: 66.714%,  Test: 65.982%,  \n",
            "Epoch: 70, Loss: 3.013,  Train: 67.314%,  Valid: 66.660%,  Test: 65.860%,  \n",
            "Epoch: 71, Loss: 3.011,  Train: 67.580%,  Valid: 66.965%,  Test: 66.144%,  \n",
            "Epoch: 72, Loss: 3.011,  Train: 67.652%,  Valid: 67.274%,  Test: 66.393%,  \n",
            "Epoch: 73, Loss: 3.010,  Train: 67.751%,  Valid: 67.331%,  Test: 66.500%,  \n",
            "Epoch: 74, Loss: 3.008,  Train: 67.951%,  Valid: 67.412%,  Test: 66.687%,  \n",
            "Epoch: 75, Loss: 3.007,  Train: 68.389%,  Valid: 67.784%,  Test: 66.895%,  \n",
            "Epoch: 76, Loss: 3.006,  Train: 68.279%,  Valid: 67.747%,  Test: 66.932%,  \n",
            "Epoch: 77, Loss: 3.005,  Train: 68.397%,  Valid: 67.996%,  Test: 67.397%,  \n",
            "Epoch: 78, Loss: 3.004,  Train: 68.518%,  Valid: 68.009%,  Test: 67.235%,  \n",
            "Epoch: 79, Loss: 3.003,  Train: 68.739%,  Valid: 68.009%,  Test: 67.292%,  \n",
            "Epoch: 80, Loss: 3.002,  Train: 68.842%,  Valid: 68.197%,  Test: 67.681%,  \n",
            "Epoch: 81, Loss: 3.001,  Train: 68.888%,  Valid: 68.096%,  Test: 67.551%,  \n",
            "Epoch: 82, Loss: 3.000,  Train: 69.133%,  Valid: 68.338%,  Test: 67.671%,  \n",
            "Epoch: 83, Loss: 2.999,  Train: 69.120%,  Valid: 68.358%,  Test: 67.615%,  \n",
            "Epoch: 84, Loss: 2.998,  Train: 69.211%,  Valid: 68.123%,  Test: 67.648%,  \n",
            "Epoch: 85, Loss: 2.997,  Train: 69.271%,  Valid: 67.959%,  Test: 67.640%,  \n",
            "Epoch: 86, Loss: 2.996,  Train: 69.090%,  Valid: 68.341%,  Test: 67.648%,  \n",
            "Epoch: 87, Loss: 2.995,  Train: 69.248%,  Valid: 68.274%,  Test: 67.601%,  \n",
            "Epoch: 88, Loss: 2.993,  Train: 69.320%,  Valid: 68.418%,  Test: 67.662%,  \n",
            "Epoch: 89, Loss: 2.993,  Train: 69.525%,  Valid: 68.328%,  Test: 67.502%,  \n",
            "Epoch: 90, Loss: 2.992,  Train: 69.463%,  Valid: 68.462%,  Test: 67.609%,  \n",
            "Epoch: 91, Loss: 2.991,  Train: 69.511%,  Valid: 68.462%,  Test: 67.874%,  \n",
            "Epoch: 92, Loss: 2.990,  Train: 69.553%,  Valid: 68.241%,  Test: 67.821%,  \n",
            "Epoch: 93, Loss: 2.990,  Train: 69.655%,  Valid: 68.304%,  Test: 67.831%,  \n",
            "Epoch: 94, Loss: 2.989,  Train: 69.632%,  Valid: 68.462%,  Test: 68.027%,  \n",
            "Epoch: 95, Loss: 2.987,  Train: 69.684%,  Valid: 68.502%,  Test: 68.014%,  \n",
            "Epoch: 96, Loss: 2.986,  Train: 69.819%,  Valid: 68.663%,  Test: 67.901%,  \n",
            "Epoch: 97, Loss: 2.985,  Train: 69.996%,  Valid: 68.345%,  Test: 67.695%,  \n",
            "Epoch: 98, Loss: 2.985,  Train: 70.184%,  Valid: 68.422%,  Test: 67.702%,  \n",
            "Epoch: 99, Loss: 2.984,  Train: 70.235%,  Valid: 68.479%,  Test: 68.234%,  \n",
            "Epoch: 100, Loss: 2.983,  Train: 70.329%,  Valid: 68.717%,  Test: 68.292%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSIx_dZAzjLG",
        "outputId": "e1c1b6d1-9663-4169-e0c6-8f6335d71cd5"
      },
      "source": [
        "print_model_accuracy(best_sagenorm, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.93%, Valid: 70.13% Test: 68.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoETpGZQvPQ1"
      },
      "source": [
        "# GAT\n",
        "Scales badly need to decrease hidden_dim, otherwise cuda goes out of memory.\n",
        "See this: https://github.com/pyg-team/pytorch_geometric/issues/527"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd_zrnm3ujHe"
      },
      "source": [
        "class GAT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads,\n",
        "                 num_layers, dropout):\n",
        "      \n",
        "      super(GAT, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [GATConv(input_dim, hidden_dim, heads, concat=False)] +\n",
        "          [GATConv(hidden_dim, hidden_dim, heads, concat=False) for _ in range(num_layers - 2)]+\n",
        "          [GATConv(hidden_dim, output_dim, heads, concat=False)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "TLFdIsMwvpep",
        "outputId": "83d896a1-93d6-4aed-c4ad-654347dc1dc2"
      },
      "source": [
        "gat = GAT(input_dim=args[\"input_dim\"], hidden_dim=16,#args[\"hidden_dim\"], \n",
        "          output_dim=args[\"output_dim\"], heads=2,#args[\"heads\"],\n",
        "          num_layers=args[\"num_layers\"], dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(gat.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_gat, gat_best_acc = train_loop(gat.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8fa24b1ebbef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                       dataset_evaluator)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-2476ca512908>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, data, optimizer, loss_fn, epochs, split_idx, evaluator)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-48f1c4f14002>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, loss_fn, optimizer, train_idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-63c979b96503>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj_t)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor, alpha: OptPairTensor, edge_attr: OptTensor)  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         out = self.propagate(edge_index, x=x, alpha=alpha, edge_attr=edge_attr,\n\u001b[0;32m--> 219\u001b[0;31m                              size=size)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                         \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_j, alpha_j, alpha_i, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m  \u001b[0;31m# Save for later use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 11.17 GiB total capacity; 10.48 GiB already allocated; 22.81 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_GZGfAn7_Xe"
      },
      "source": [
        "## Stacking multiple layers together\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4r5iZKav-A3"
      },
      "source": [
        "from torch_geometric.utils import to_networkx, add_self_loops\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJzeX0_P8Mru",
        "outputId": "e17b0ae8-09e6-4146-d0c5-7d4180b0403f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Too slow\n",
        "#net = nx.from_scipy_sparse_matrix(data.adj_t.to_scipy())\n",
        "#d = nx.algorithms.distance_measures.diameter(net)\n",
        "#print(f\"Graph diameter is {d}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_sparse/storage.py:14: UserWarning: `layout` argument unset, using default layout \"coo\". This may lead to unexpected behaviour.\n",
            "  warnings.warn('`layout` argument unset, using default layout '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCeE7t39MjVb",
        "outputId": "c952c89f-3777-40b2-937f-3eb369b3c39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "todata.adj_t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([     0,      0,      0,  ..., 169341, 169342, 169342], device='cuda:0'),\n",
              "             col=tensor([   411,    640,   1162,  ..., 163274,  27824, 158981], device='cuda:0'),\n",
              "             size=(169343, 169343), nnz=2315598, density=0.01%)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMxXxDmgCuvA"
      },
      "source": [
        "## Stacking multiple linear layers\n",
        "* Message passing composed of n linear layers instead of one\n",
        "* Aggregation can be a mlp\n",
        "\n",
        "**Question** in aggregation how do we mantain the order invariant property?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJkineSJENov"
      },
      "source": [
        "import itertools\n",
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.utils import add_self_loops, degree, contains_self_loops\n",
        "from torch_geometric.nn.inits import zeros\n",
        "\n",
        "\n",
        "class DeepGCNConv(GCNConv):\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 improved: bool = False, cached: bool = False,\n",
        "                 add_self_loops: bool = True, normalize: bool = True,\n",
        "                 bias: bool = True, num_msg_layers: int = 1, \n",
        "                 num_agg_layers: int = 0,\n",
        "                 **kwargs):\n",
        "      \n",
        "        super().__init__(in_channels, out_channels, improved, cached, \n",
        "                         add_self_loops, normalize, bias, **kwargs)  \n",
        "\n",
        "        self.num_msg_layers = num_msg_layers\n",
        "        self.num_agg_layers = num_agg_layers\n",
        "\n",
        "        if self.num_msg_layers > 1:\n",
        "          mlp_start = [torch.nn.Linear(in_channels, out_channels), torch.nn.ReLU()]\n",
        "          mlp = list(\n",
        "              itertools.chain.from_iterable(\n",
        "              [[torch.nn.Linear(out_channels, out_channels), torch.nn.ReLU()]\n",
        "              for i in range(self.num_msg_layers - 2)])\n",
        "              )\n",
        "          mlp_end = [torch.nn.Linear(out_channels, out_channels)]\n",
        "          self.lin = torch.nn.Sequential(*(mlp_start + mlp + mlp_end))\n",
        "\n",
        "        if self.num_agg_layers == 1:\n",
        "          self.lin_agg = torch.nn.Linear(out_channels, out_channels)\n",
        "        elif self.num_agg_layers > 1:\n",
        "          mlp_start = [torch.nn.Linear(out_channels, out_channels), torch.nn.ReLU()]\n",
        "          mlp = list(\n",
        "              itertools.chain.from_iterable(\n",
        "              [[torch.nn.Linear(out_channels, out_channels), torch.nn.ReLU()]\n",
        "              for i in range(self.num_agg_layers - 2)])\n",
        "              )\n",
        "          mlp_end = [torch.nn.Linear(out_channels, out_channels)]\n",
        "          self.lin_agg = torch.nn.Sequential(*(mlp_start + mlp + mlp_end))\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        if self.num_agg_layers > 0:\n",
        "          out = matmul(adj_t, x, reduce=self.aggr)\n",
        "          return self.lin_agg(out)\n",
        "        else:\n",
        "          return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #self.lin.reset_parameters()\n",
        "        zeros(self.bias)\n",
        "        self._cached_edge_index = None\n",
        "        self._cached_adj_t = None"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUtz-4RN-QvT"
      },
      "source": [
        "class DeepGCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, \n",
        "                 num_msg_layers, num_agg_layers, num_layers, dropout):\n",
        "      \n",
        "      super(DeepGCN, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [DeepGCNConv(input_dim, hidden_dim, num_msg_layers=num_msg_layers,\n",
        "                       num_agg_layers=num_agg_layers)] +\n",
        "          [DeepGCNConv(hidden_dim, hidden_dim, num_msg_layers=num_msg_layers, \n",
        "                       num_agg_layers=num_agg_layers) \n",
        "           for _ in range(num_layers - 2)]+\n",
        "          [DeepGCNConv(hidden_dim, output_dim, num_msg_layers=num_msg_layers, \n",
        "                       num_agg_layers=num_agg_layers)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 layer mlp in message passing"
      ],
      "metadata": {
        "id": "fTLexjg3JGil"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09vb8v3g8X09",
        "outputId": "a355a217-1ec8-4425-a3b7-9ed369470035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "source": [
        "dgcn = DeepGCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"],\n",
        "               output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"],\n",
        "               num_msg_layers=3, num_agg_layers=0, dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(dgcn.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_dgcn, dgcn_best_acc = train_loop(dgcn.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.650,  Train: 4.766%,  Valid: 9.561%,  Test: 9.528%,  \n",
            "Epoch: 02, Loss: 3.378,  Train: 18.950%,  Valid: 25.746%,  Test: 23.449%,  \n",
            "Epoch: 03, Loss: 2.975,  Train: 17.906%,  Valid: 7.628%,  Test: 5.862%,  \n",
            "Epoch: 04, Loss: 3.050,  Train: 11.190%,  Valid: 23.021%,  Test: 21.581%,  \n",
            "Epoch: 05, Loss: 2.736,  Train: 13.210%,  Valid: 23.779%,  Test: 22.025%,  \n",
            "Epoch: 06, Loss: 2.637,  Train: 28.025%,  Valid: 31.451%,  Test: 34.304%,  \n",
            "Epoch: 07, Loss: 2.607,  Train: 23.661%,  Valid: 15.232%,  Test: 14.219%,  \n",
            "Epoch: 08, Loss: 2.517,  Train: 26.104%,  Valid: 25.789%,  Test: 25.252%,  \n",
            "Epoch: 09, Loss: 2.478,  Train: 24.761%,  Valid: 23.511%,  Test: 22.624%,  \n",
            "Epoch: 10, Loss: 2.426,  Train: 23.417%,  Valid: 17.464%,  Test: 16.824%,  \n",
            "Epoch: 11, Loss: 2.352,  Train: 28.838%,  Valid: 21.561%,  Test: 20.497%,  \n",
            "Epoch: 12, Loss: 2.285,  Train: 27.838%,  Valid: 19.021%,  Test: 17.701%,  \n",
            "Epoch: 13, Loss: 2.228,  Train: 39.093%,  Valid: 42.528%,  Test: 39.631%,  \n",
            "Epoch: 14, Loss: 2.187,  Train: 32.378%,  Valid: 25.209%,  Test: 22.511%,  \n",
            "Epoch: 15, Loss: 2.126,  Train: 36.305%,  Valid: 34.256%,  Test: 31.362%,  \n",
            "Epoch: 16, Loss: 2.071,  Train: 38.361%,  Valid: 40.790%,  Test: 38.275%,  \n",
            "Epoch: 17, Loss: 2.019,  Train: 37.352%,  Valid: 36.669%,  Test: 33.916%,  \n",
            "Epoch: 18, Loss: 1.983,  Train: 41.131%,  Valid: 44.478%,  Test: 42.656%,  \n",
            "Epoch: 19, Loss: 1.942,  Train: 37.982%,  Valid: 42.189%,  Test: 41.753%,  \n",
            "Epoch: 20, Loss: 1.913,  Train: 40.593%,  Valid: 46.092%,  Test: 46.382%,  \n",
            "Epoch: 21, Loss: 1.883,  Train: 42.958%,  Valid: 49.196%,  Test: 50.633%,  \n",
            "Epoch: 22, Loss: 1.843,  Train: 37.584%,  Valid: 41.146%,  Test: 41.997%,  \n",
            "Epoch: 23, Loss: 1.821,  Train: 37.308%,  Valid: 39.216%,  Test: 38.932%,  \n",
            "Epoch: 24, Loss: 1.795,  Train: 41.021%,  Valid: 43.951%,  Test: 43.689%,  \n",
            "Epoch: 25, Loss: 1.771,  Train: 42.797%,  Valid: 46.119%,  Test: 45.073%,  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-db21240994c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                       dataset_evaluator)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-6acd17c359b4>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, data, optimizer, loss_fn, epochs, split_idx, evaluator)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-48f1c4f14002>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, loss_fn, optimizer, train_idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7vfAYeJGO1n"
      },
      "source": [
        "print_model_accuracy(best_sage, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}