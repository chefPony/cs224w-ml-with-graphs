{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Neural Networks: Design Space.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNciVKFjpHmv4RtSTT0thpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefPony/cs224w-ml-with-graphs/blob/main/Graph_Neural_Networks_Design_Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tZ_uWPluKeJ",
        "outputId": "a7001731-3650-44f2-d228-8bdeae574757"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p983o-Uf3goM",
        "outputId": "708cd916-ecaa-4472-951c-6442b9d0ee1f"
      },
      "source": [
        "# Install torch geometric\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.12\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=afdd7b889848aa1830c33fced6ceef625fb20fe63fe0364137967bb773bb42ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, yacs, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.1)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=8c94a3af74bf164a51800cd4a89c7a095e4396cb09f1fb5cb457ce5daeef3a5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.2 outdated-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh9CqxAr3rMf",
        "outputId": "43df5b58-3879-4fcc-ec38-bb6948be02d4"
      },
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  # Load the dataset and transform it to sparse tensor\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "  # Extract the graph\n",
        "  data = dataset[0]\n",
        "  print(data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:15<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 6689.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1691.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiP3NGiD38Z7",
        "outputId": "97314a51-ad33-4dca-fe62-2ad7069070ff"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyVZ_4lw4MxH",
        "outputId": "76f331f3-bc6e-4cac-b866-db0d28f6fae3"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  data = dataset[0]\n",
        "\n",
        "  # Make the adjacency matrix to symmetric\n",
        "  data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  # If you use GPU, the device should be cuda\n",
        "  print('Device: {}'.format(device))\n",
        "\n",
        "  data = data.to(device)\n",
        "  split_idx = dataset.get_idx_split()\n",
        "  train_idx = split_idx['train'].to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBQZRx5t8KS3"
      },
      "source": [
        "def train(model, data, loss_fn, optimizer, train_idx):\n",
        "    \n",
        "  model.train()\n",
        "  loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  y_pred = model(data.x, data.adj_t)\n",
        "  loss = loss_fn(y_pred[train_idx], data.y[train_idx].squeeze())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z-SUCMS83SU"
      },
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  y_pred = model(data.x, data.adj_t)\n",
        "  y_pred = y_pred.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "  train_acc = evaluator.eval({\n",
        "      \"y_pred\": y_pred[split_idx[\"train\"]],\n",
        "      \"y_true\": data.y[split_idx[\"train\"]]\n",
        "  })[\"acc\"]\n",
        "  val_acc = evaluator.eval({\n",
        "    \"y_pred\": y_pred[split_idx[\"valid\"]],\n",
        "    \"y_true\": data.y[split_idx[\"valid\"]]\n",
        "  })[\"acc\"]\n",
        "  test_acc = evaluator.eval({\n",
        "    \"y_pred\": y_pred[split_idx[\"test\"]],\n",
        "    \"y_true\": data.y[split_idx[\"test\"]]\n",
        "  })[\"acc\"]\n",
        "\n",
        "  return train_acc, val_acc, test_acc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFgcuHT5-9Br"
      },
      "source": [
        "def train_loop(model, data, optimizer, loss_fn, epochs, split_idx, evaluator):\n",
        "\n",
        "  best_val_acc = 0.\n",
        "\n",
        "  model.reset_parameters()\n",
        "\n",
        "  for e in range(1, epochs+1):\n",
        "    loss = train(model, data, loss_fn, optimizer, split_idx[\"train\"])\n",
        "    train_acc, val_acc, test_acc = test(model, data, split_idx, evaluator)\n",
        "\n",
        "    if val_acc>best_val_acc:\n",
        "      best_model = copy.deepcopy(model)\n",
        "      best_val_acc = val_acc\n",
        "\n",
        "    print(f\"Epoch: {e:02d}, Loss: {loss:.3f},  \"\n",
        "        f\"Train: {100 * train_acc:.3f}%,  \"   \n",
        "        f\"Valid: {100 * val_acc:.3f}%,  \" \n",
        "        f\"Test: {100 * test_acc:.3f}%,  \"\n",
        "        )\n",
        "  return best_model, best_val_acc\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckQ_JxV6tv94"
      },
      "source": [
        "def print_model_accuracy(best_model, data, split_idx, evaluator):\n",
        "  best_result = test(best_model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzlvSjiuNFZ"
      },
      "source": [
        "dataset_evaluator = Evaluator(name=dataset_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Dwgasx-Np2"
      },
      "source": [
        "args = {\n",
        "    \"input_dim\": data.num_features,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"output_dim\": dataset.num_classes,\n",
        "    \"num_layers\": 3,\n",
        "    \"heads\": 2,\n",
        "    \"dropout\": 0.5,\n",
        "    \"epochs\": 100,\n",
        "    \"lr\": 0.01\n",
        "}\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW40CRXupLuR"
      },
      "source": [
        "## Vanilla GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92LyQhFB6Qrf"
      },
      "source": [
        "class GCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_preprocess_layers, \n",
        "                 num_postprocess_layers, num_gcn_layers, dropout):\n",
        "      \n",
        "      super(GCN, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      self.num_preprocess_layers = num_preprocess_layers\n",
        "      self.num_postprocess_layers = num_postprocess_layers\n",
        "\n",
        "      if self.num_preprocess_layers > 0:\n",
        "        mlp = list(\n",
        "            itertools.chain.from_iterable(\n",
        "            [[torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU()]]+\n",
        "            [[torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU()]\n",
        "            for i in range(num_preprocess_layers - 1)])\n",
        "            )\n",
        "        self.preprocess = torch.nn.Sequential(*mlp)\n",
        "\n",
        "      gcn_input_dim = hidden_dim if self.num_preprocess_layers > 0 else input_dim\n",
        "      gcn_output_dim = hidden_dim if self.num_postprocess_layers > 0 else output_dim\n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [GCNConv(gcn_input_dim, hidden_dim)] +\n",
        "          [GCNConv(hidden_dim, hidden_dim) for _ in range(num_gcn_layers - 2)]+\n",
        "          [GCNConv(hidden_dim, gcn_output_dim)]\n",
        "      )\n",
        "\n",
        "      if self.num_postprocess_layers > 0:\n",
        "        mlp_end = [torch.nn.Linear(hidden_dim, output_dim)]\n",
        "        mlp = list(\n",
        "            itertools.chain.from_iterable(\n",
        "            [[torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU()]\n",
        "            for i in range(num_preprocess_layers - 1)])\n",
        "            )\n",
        "        self.postprocess = torch.nn.Sequential(*(mlp + mlp_end))\n",
        "\n",
        "      num_bns = num_gcn_layers + min(num_postprocess_layers, 1) - 1\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_gcn_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "      out = x\n",
        "      if self.num_preprocess_layers > 0:\n",
        "        out = self.preprocess(out)\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      if self.num_postprocess_layers > 0:\n",
        "        out = self.bns[-1](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.postprocess(out)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFyB_q-rqAE5",
        "outputId": "78332fda-53e7-490c-a4c0-64863f321678"
      },
      "source": [
        "gcn = GCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "          output_dim=args[\"output_dim\"], num_gcn_layers=args[\"num_layers\"],\n",
        "          num_preprocess_layers=0, num_postprocess_layers=0, \n",
        "          dropout=args[\"dropout\"])\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(gcn.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_gcn, gcn_best_acc = train_loop(gcn.to(device), \n",
        "                                    data, optimizer, nll_loss, \n",
        "                                    args[\"epochs\"], split_idx, \n",
        "                                    dataset_evaluator)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.019,  Train: 17.555%,  Valid: 25.239%,  Test: 23.034%,  \n",
            "Epoch: 02, Loss: 2.423,  Train: 29.709%,  Valid: 31.655%,  Test: 36.983%,  \n",
            "Epoch: 03, Loss: 1.950,  Train: 28.744%,  Valid: 28.115%,  Test: 33.576%,  \n",
            "Epoch: 04, Loss: 1.805,  Train: 35.909%,  Valid: 35.914%,  Test: 41.633%,  \n",
            "Epoch: 05, Loss: 1.652,  Train: 38.093%,  Valid: 37.250%,  Test: 41.921%,  \n",
            "Epoch: 06, Loss: 1.583,  Train: 38.756%,  Valid: 31.568%,  Test: 33.385%,  \n",
            "Epoch: 07, Loss: 1.500,  Train: 39.985%,  Valid: 30.676%,  Test: 31.953%,  \n",
            "Epoch: 08, Loss: 1.446,  Train: 40.099%,  Valid: 32.008%,  Test: 34.500%,  \n",
            "Epoch: 09, Loss: 1.409,  Train: 40.124%,  Valid: 32.501%,  Test: 35.992%,  \n",
            "Epoch: 10, Loss: 1.370,  Train: 41.579%,  Valid: 35.354%,  Test: 39.113%,  \n",
            "Epoch: 11, Loss: 1.338,  Train: 43.041%,  Valid: 37.609%,  Test: 41.444%,  \n",
            "Epoch: 12, Loss: 1.311,  Train: 43.967%,  Valid: 39.132%,  Test: 42.199%,  \n",
            "Epoch: 13, Loss: 1.287,  Train: 43.481%,  Valid: 39.102%,  Test: 41.833%,  \n",
            "Epoch: 14, Loss: 1.263,  Train: 43.478%,  Valid: 40.726%,  Test: 42.765%,  \n",
            "Epoch: 15, Loss: 1.239,  Train: 43.767%,  Valid: 41.582%,  Test: 45.464%,  \n",
            "Epoch: 16, Loss: 1.230,  Train: 46.601%,  Valid: 46.451%,  Test: 50.310%,  \n",
            "Epoch: 17, Loss: 1.213,  Train: 49.773%,  Valid: 50.401%,  Test: 53.733%,  \n",
            "Epoch: 18, Loss: 1.196,  Train: 53.507%,  Valid: 54.233%,  Test: 57.091%,  \n",
            "Epoch: 19, Loss: 1.188,  Train: 55.664%,  Valid: 56.401%,  Test: 58.937%,  \n",
            "Epoch: 20, Loss: 1.173,  Train: 57.125%,  Valid: 57.881%,  Test: 59.729%,  \n",
            "Epoch: 21, Loss: 1.163,  Train: 57.157%,  Valid: 58.452%,  Test: 60.040%,  \n",
            "Epoch: 22, Loss: 1.154,  Train: 56.704%,  Valid: 57.475%,  Test: 59.301%,  \n",
            "Epoch: 23, Loss: 1.143,  Train: 56.351%,  Valid: 56.220%,  Test: 58.478%,  \n",
            "Epoch: 24, Loss: 1.139,  Train: 56.753%,  Valid: 55.626%,  Test: 57.692%,  \n",
            "Epoch: 25, Loss: 1.128,  Train: 57.858%,  Valid: 56.535%,  Test: 58.426%,  \n",
            "Epoch: 26, Loss: 1.121,  Train: 59.209%,  Valid: 58.438%,  Test: 59.799%,  \n",
            "Epoch: 27, Loss: 1.116,  Train: 60.921%,  Valid: 61.099%,  Test: 62.033%,  \n",
            "Epoch: 28, Loss: 1.106,  Train: 62.285%,  Valid: 62.401%,  Test: 63.564%,  \n",
            "Epoch: 29, Loss: 1.102,  Train: 63.297%,  Valid: 63.358%,  Test: 64.609%,  \n",
            "Epoch: 30, Loss: 1.095,  Train: 64.214%,  Valid: 64.375%,  Test: 65.284%,  \n",
            "Epoch: 31, Loss: 1.087,  Train: 65.120%,  Valid: 65.311%,  Test: 65.790%,  \n",
            "Epoch: 32, Loss: 1.082,  Train: 65.600%,  Valid: 65.808%,  Test: 65.994%,  \n",
            "Epoch: 33, Loss: 1.076,  Train: 66.267%,  Valid: 66.405%,  Test: 66.416%,  \n",
            "Epoch: 34, Loss: 1.073,  Train: 66.461%,  Valid: 66.032%,  Test: 66.521%,  \n",
            "Epoch: 35, Loss: 1.065,  Train: 66.579%,  Valid: 66.455%,  Test: 66.912%,  \n",
            "Epoch: 36, Loss: 1.062,  Train: 66.831%,  Valid: 67.009%,  Test: 67.128%,  \n",
            "Epoch: 37, Loss: 1.055,  Train: 66.986%,  Valid: 66.683%,  Test: 67.068%,  \n",
            "Epoch: 38, Loss: 1.054,  Train: 67.360%,  Valid: 67.482%,  Test: 67.315%,  \n",
            "Epoch: 39, Loss: 1.047,  Train: 67.603%,  Valid: 67.613%,  Test: 67.572%,  \n",
            "Epoch: 40, Loss: 1.044,  Train: 68.085%,  Valid: 67.556%,  Test: 67.741%,  \n",
            "Epoch: 41, Loss: 1.037,  Train: 68.186%,  Valid: 67.811%,  Test: 68.004%,  \n",
            "Epoch: 42, Loss: 1.033,  Train: 68.426%,  Valid: 68.254%,  Test: 67.953%,  \n",
            "Epoch: 43, Loss: 1.031,  Train: 68.643%,  Valid: 68.183%,  Test: 68.049%,  \n",
            "Epoch: 44, Loss: 1.029,  Train: 68.715%,  Valid: 68.298%,  Test: 68.000%,  \n",
            "Epoch: 45, Loss: 1.022,  Train: 68.946%,  Valid: 68.673%,  Test: 68.300%,  \n",
            "Epoch: 46, Loss: 1.024,  Train: 69.024%,  Valid: 68.898%,  Test: 68.399%,  \n",
            "Epoch: 47, Loss: 1.017,  Train: 69.051%,  Valid: 68.730%,  Test: 68.218%,  \n",
            "Epoch: 48, Loss: 1.017,  Train: 69.284%,  Valid: 68.616%,  Test: 67.786%,  \n",
            "Epoch: 49, Loss: 1.011,  Train: 69.073%,  Valid: 68.241%,  Test: 67.053%,  \n",
            "Epoch: 50, Loss: 1.013,  Train: 69.165%,  Valid: 68.546%,  Test: 67.755%,  \n",
            "Epoch: 51, Loss: 1.005,  Train: 69.254%,  Valid: 68.831%,  Test: 68.257%,  \n",
            "Epoch: 52, Loss: 1.005,  Train: 69.299%,  Valid: 68.932%,  Test: 68.481%,  \n",
            "Epoch: 53, Loss: 1.002,  Train: 69.431%,  Valid: 68.922%,  Test: 68.397%,  \n",
            "Epoch: 54, Loss: 0.998,  Train: 69.629%,  Valid: 68.868%,  Test: 67.924%,  \n",
            "Epoch: 55, Loss: 0.999,  Train: 69.477%,  Valid: 68.620%,  Test: 67.986%,  \n",
            "Epoch: 56, Loss: 0.994,  Train: 69.387%,  Valid: 68.730%,  Test: 68.101%,  \n",
            "Epoch: 57, Loss: 0.992,  Train: 69.588%,  Valid: 68.882%,  Test: 68.136%,  \n",
            "Epoch: 58, Loss: 0.987,  Train: 69.909%,  Valid: 68.747%,  Test: 67.794%,  \n",
            "Epoch: 59, Loss: 0.987,  Train: 69.898%,  Valid: 68.808%,  Test: 67.292%,  \n",
            "Epoch: 60, Loss: 0.981,  Train: 70.023%,  Valid: 69.288%,  Test: 67.642%,  \n",
            "Epoch: 61, Loss: 0.980,  Train: 70.020%,  Valid: 69.422%,  Test: 68.819%,  \n",
            "Epoch: 62, Loss: 0.979,  Train: 69.690%,  Valid: 69.180%,  Test: 69.138%,  \n",
            "Epoch: 63, Loss: 0.975,  Train: 70.075%,  Valid: 69.288%,  Test: 69.331%,  \n",
            "Epoch: 64, Loss: 0.973,  Train: 70.412%,  Valid: 69.261%,  Test: 68.438%,  \n",
            "Epoch: 65, Loss: 0.972,  Train: 70.301%,  Valid: 68.660%,  Test: 66.934%,  \n",
            "Epoch: 66, Loss: 0.972,  Train: 70.275%,  Valid: 68.975%,  Test: 67.471%,  \n",
            "Epoch: 67, Loss: 0.967,  Train: 70.539%,  Valid: 69.536%,  Test: 68.193%,  \n",
            "Epoch: 68, Loss: 0.966,  Train: 70.671%,  Valid: 69.744%,  Test: 69.074%,  \n",
            "Epoch: 69, Loss: 0.960,  Train: 70.802%,  Valid: 69.982%,  Test: 69.485%,  \n",
            "Epoch: 70, Loss: 0.959,  Train: 71.013%,  Valid: 70.368%,  Test: 69.765%,  \n",
            "Epoch: 71, Loss: 0.960,  Train: 70.834%,  Valid: 70.180%,  Test: 69.893%,  \n",
            "Epoch: 72, Loss: 0.956,  Train: 70.845%,  Valid: 70.220%,  Test: 69.932%,  \n",
            "Epoch: 73, Loss: 0.955,  Train: 70.880%,  Valid: 69.848%,  Test: 69.111%,  \n",
            "Epoch: 74, Loss: 0.952,  Train: 71.025%,  Valid: 69.754%,  Test: 68.722%,  \n",
            "Epoch: 75, Loss: 0.951,  Train: 70.928%,  Valid: 69.415%,  Test: 68.383%,  \n",
            "Epoch: 76, Loss: 0.950,  Train: 70.871%,  Valid: 69.482%,  Test: 68.525%,  \n",
            "Epoch: 77, Loss: 0.948,  Train: 70.854%,  Valid: 69.976%,  Test: 69.331%,  \n",
            "Epoch: 78, Loss: 0.947,  Train: 71.011%,  Valid: 70.177%,  Test: 69.348%,  \n",
            "Epoch: 79, Loss: 0.941,  Train: 71.166%,  Valid: 70.130%,  Test: 69.062%,  \n",
            "Epoch: 80, Loss: 0.941,  Train: 71.001%,  Valid: 70.368%,  Test: 69.632%,  \n",
            "Epoch: 81, Loss: 0.939,  Train: 71.296%,  Valid: 70.385%,  Test: 70.043%,  \n",
            "Epoch: 82, Loss: 0.938,  Train: 71.428%,  Valid: 70.157%,  Test: 69.529%,  \n",
            "Epoch: 83, Loss: 0.936,  Train: 71.281%,  Valid: 69.667%,  Test: 68.088%,  \n",
            "Epoch: 84, Loss: 0.935,  Train: 71.219%,  Valid: 69.714%,  Test: 68.360%,  \n",
            "Epoch: 85, Loss: 0.934,  Train: 71.232%,  Valid: 69.613%,  Test: 68.496%,  \n",
            "Epoch: 86, Loss: 0.929,  Train: 71.325%,  Valid: 69.794%,  Test: 69.341%,  \n",
            "Epoch: 87, Loss: 0.930,  Train: 71.412%,  Valid: 70.083%,  Test: 69.465%,  \n",
            "Epoch: 88, Loss: 0.928,  Train: 71.141%,  Valid: 70.170%,  Test: 69.681%,  \n",
            "Epoch: 89, Loss: 0.928,  Train: 71.254%,  Valid: 70.046%,  Test: 69.514%,  \n",
            "Epoch: 90, Loss: 0.927,  Train: 71.441%,  Valid: 70.318%,  Test: 69.788%,  \n",
            "Epoch: 91, Loss: 0.924,  Train: 71.454%,  Valid: 69.996%,  Test: 69.210%,  \n",
            "Epoch: 92, Loss: 0.925,  Train: 71.531%,  Valid: 70.033%,  Test: 69.302%,  \n",
            "Epoch: 93, Loss: 0.920,  Train: 71.344%,  Valid: 70.063%,  Test: 69.399%,  \n",
            "Epoch: 94, Loss: 0.921,  Train: 71.569%,  Valid: 70.301%,  Test: 69.636%,  \n",
            "Epoch: 95, Loss: 0.917,  Train: 71.566%,  Valid: 69.680%,  Test: 68.049%,  \n",
            "Epoch: 96, Loss: 0.917,  Train: 71.798%,  Valid: 70.553%,  Test: 69.592%,  \n",
            "Epoch: 97, Loss: 0.917,  Train: 71.421%,  Valid: 70.123%,  Test: 70.191%,  \n",
            "Epoch: 98, Loss: 0.915,  Train: 71.605%,  Valid: 70.210%,  Test: 69.095%,  \n",
            "Epoch: 99, Loss: 0.912,  Train: 71.497%,  Valid: 68.972%,  Test: 66.891%,  \n",
            "Epoch: 100, Loss: 0.913,  Train: 71.926%,  Valid: 70.435%,  Test: 69.494%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7qxVZVdyYSX",
        "outputId": "af887f43-e512-42f3-ef81-4d14cf2a6a77"
      },
      "source": [
        "print_model_accuracy(best_gcn, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.63%, Valid: 70.33% Test: 69.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49hK34vUpSyN"
      },
      "source": [
        "## GraphSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKsTeokbCcE"
      },
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout,\n",
        "                 normalize=False):\n",
        "      \n",
        "      super(SAGE, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [SAGEConv(input_dim, hidden_dim, normalize=normalize)] +\n",
        "          [SAGEConv(hidden_dim, hidden_dim, normalize=normalize) \n",
        "           for _ in range(num_layers - 2)]+\n",
        "          [SAGEConv(hidden_dim, output_dim, normalize=normalize)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLAwUs5vwrj3"
      },
      "source": [
        "### No normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIH9szzcuV7e",
        "outputId": "abbff3f9-624c-4618-a75a-0cd04801a4b3"
      },
      "source": [
        "sage = SAGE(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "            output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "            dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(sage.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sage, sage_best_acc = train_loop(sage.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.943,  Train: 31.739%,  Valid: 33.303%,  Test: 32.663%,  \n",
            "Epoch: 02, Loss: 2.582,  Train: 29.385%,  Valid: 33.649%,  Test: 32.541%,  \n",
            "Epoch: 03, Loss: 2.220,  Train: 35.980%,  Valid: 41.589%,  Test: 44.392%,  \n",
            "Epoch: 04, Loss: 2.047,  Train: 40.289%,  Valid: 44.793%,  Test: 46.409%,  \n",
            "Epoch: 05, Loss: 1.825,  Train: 44.133%,  Valid: 46.693%,  Test: 46.773%,  \n",
            "Epoch: 06, Loss: 1.688,  Train: 47.782%,  Valid: 50.203%,  Test: 50.022%,  \n",
            "Epoch: 07, Loss: 1.617,  Train: 49.836%,  Valid: 53.646%,  Test: 54.021%,  \n",
            "Epoch: 08, Loss: 1.539,  Train: 50.304%,  Valid: 54.066%,  Test: 54.256%,  \n",
            "Epoch: 09, Loss: 1.485,  Train: 51.090%,  Valid: 54.562%,  Test: 54.404%,  \n",
            "Epoch: 10, Loss: 1.448,  Train: 52.172%,  Valid: 54.938%,  Test: 54.799%,  \n",
            "Epoch: 11, Loss: 1.416,  Train: 53.626%,  Valid: 56.767%,  Test: 56.745%,  \n",
            "Epoch: 12, Loss: 1.384,  Train: 55.081%,  Valid: 58.190%,  Test: 58.361%,  \n",
            "Epoch: 13, Loss: 1.361,  Train: 56.581%,  Valid: 59.713%,  Test: 59.982%,  \n",
            "Epoch: 14, Loss: 1.346,  Train: 58.020%,  Valid: 60.740%,  Test: 61.373%,  \n",
            "Epoch: 15, Loss: 1.323,  Train: 58.915%,  Valid: 62.069%,  Test: 62.313%,  \n",
            "Epoch: 16, Loss: 1.300,  Train: 59.590%,  Valid: 62.143%,  Test: 62.556%,  \n",
            "Epoch: 17, Loss: 1.276,  Train: 60.202%,  Valid: 62.549%,  Test: 62.572%,  \n",
            "Epoch: 18, Loss: 1.258,  Train: 60.560%,  Valid: 62.831%,  Test: 62.932%,  \n",
            "Epoch: 19, Loss: 1.245,  Train: 61.054%,  Valid: 63.368%,  Test: 63.912%,  \n",
            "Epoch: 20, Loss: 1.232,  Train: 61.410%,  Valid: 64.049%,  Test: 64.381%,  \n",
            "Epoch: 21, Loss: 1.219,  Train: 62.468%,  Valid: 64.626%,  Test: 64.967%,  \n",
            "Epoch: 22, Loss: 1.209,  Train: 62.945%,  Valid: 64.942%,  Test: 65.317%,  \n",
            "Epoch: 23, Loss: 1.200,  Train: 63.550%,  Valid: 65.422%,  Test: 65.383%,  \n",
            "Epoch: 24, Loss: 1.191,  Train: 63.883%,  Valid: 65.502%,  Test: 65.552%,  \n",
            "Epoch: 25, Loss: 1.175,  Train: 64.255%,  Valid: 65.855%,  Test: 65.445%,  \n",
            "Epoch: 26, Loss: 1.162,  Train: 64.452%,  Valid: 65.771%,  Test: 65.370%,  \n",
            "Epoch: 27, Loss: 1.154,  Train: 64.675%,  Valid: 65.979%,  Test: 65.724%,  \n",
            "Epoch: 28, Loss: 1.147,  Train: 65.250%,  Valid: 66.207%,  Test: 66.132%,  \n",
            "Epoch: 29, Loss: 1.136,  Train: 65.650%,  Valid: 66.489%,  Test: 66.132%,  \n",
            "Epoch: 30, Loss: 1.132,  Train: 66.034%,  Valid: 66.761%,  Test: 66.187%,  \n",
            "Epoch: 31, Loss: 1.124,  Train: 66.202%,  Valid: 66.845%,  Test: 66.058%,  \n",
            "Epoch: 32, Loss: 1.116,  Train: 66.342%,  Valid: 66.777%,  Test: 66.056%,  \n",
            "Epoch: 33, Loss: 1.111,  Train: 66.473%,  Valid: 67.408%,  Test: 66.541%,  \n",
            "Epoch: 34, Loss: 1.102,  Train: 66.839%,  Valid: 67.539%,  Test: 67.064%,  \n",
            "Epoch: 35, Loss: 1.095,  Train: 67.032%,  Valid: 67.724%,  Test: 67.216%,  \n",
            "Epoch: 36, Loss: 1.090,  Train: 67.157%,  Valid: 67.905%,  Test: 67.422%,  \n",
            "Epoch: 37, Loss: 1.085,  Train: 67.511%,  Valid: 67.945%,  Test: 67.590%,  \n",
            "Epoch: 38, Loss: 1.081,  Train: 67.718%,  Valid: 68.160%,  Test: 67.321%,  \n",
            "Epoch: 39, Loss: 1.072,  Train: 67.700%,  Valid: 68.116%,  Test: 67.446%,  \n",
            "Epoch: 40, Loss: 1.069,  Train: 67.911%,  Valid: 68.274%,  Test: 67.630%,  \n",
            "Epoch: 41, Loss: 1.063,  Train: 67.902%,  Valid: 68.073%,  Test: 67.720%,  \n",
            "Epoch: 42, Loss: 1.059,  Train: 68.116%,  Valid: 68.234%,  Test: 67.971%,  \n",
            "Epoch: 43, Loss: 1.053,  Train: 68.239%,  Valid: 68.381%,  Test: 68.125%,  \n",
            "Epoch: 44, Loss: 1.050,  Train: 68.215%,  Valid: 68.697%,  Test: 68.370%,  \n",
            "Epoch: 45, Loss: 1.049,  Train: 68.451%,  Valid: 68.727%,  Test: 68.272%,  \n",
            "Epoch: 46, Loss: 1.041,  Train: 68.569%,  Valid: 68.586%,  Test: 67.839%,  \n",
            "Epoch: 47, Loss: 1.037,  Train: 68.789%,  Valid: 68.912%,  Test: 68.175%,  \n",
            "Epoch: 48, Loss: 1.031,  Train: 68.812%,  Valid: 68.630%,  Test: 68.562%,  \n",
            "Epoch: 49, Loss: 1.029,  Train: 68.977%,  Valid: 68.858%,  Test: 68.658%,  \n",
            "Epoch: 50, Loss: 1.024,  Train: 68.737%,  Valid: 69.006%,  Test: 68.479%,  \n",
            "Epoch: 51, Loss: 1.022,  Train: 68.970%,  Valid: 69.086%,  Test: 68.418%,  \n",
            "Epoch: 52, Loss: 1.017,  Train: 69.100%,  Valid: 68.942%,  Test: 68.294%,  \n",
            "Epoch: 53, Loss: 1.013,  Train: 69.251%,  Valid: 68.918%,  Test: 68.761%,  \n",
            "Epoch: 54, Loss: 1.012,  Train: 69.538%,  Valid: 69.137%,  Test: 68.685%,  \n",
            "Epoch: 55, Loss: 1.007,  Train: 69.155%,  Valid: 68.892%,  Test: 68.313%,  \n",
            "Epoch: 56, Loss: 1.002,  Train: 69.541%,  Valid: 69.016%,  Test: 68.683%,  \n",
            "Epoch: 57, Loss: 1.002,  Train: 69.527%,  Valid: 69.079%,  Test: 68.652%,  \n",
            "Epoch: 58, Loss: 0.998,  Train: 69.430%,  Valid: 68.720%,  Test: 68.498%,  \n",
            "Epoch: 59, Loss: 0.994,  Train: 69.799%,  Valid: 68.928%,  Test: 68.214%,  \n",
            "Epoch: 60, Loss: 0.993,  Train: 69.762%,  Valid: 68.912%,  Test: 68.451%,  \n",
            "Epoch: 61, Loss: 0.990,  Train: 69.855%,  Valid: 69.187%,  Test: 68.551%,  \n",
            "Epoch: 62, Loss: 0.987,  Train: 69.975%,  Valid: 69.002%,  Test: 68.852%,  \n",
            "Epoch: 63, Loss: 0.986,  Train: 69.775%,  Valid: 69.113%,  Test: 68.702%,  \n",
            "Epoch: 64, Loss: 0.978,  Train: 69.882%,  Valid: 69.106%,  Test: 69.062%,  \n",
            "Epoch: 65, Loss: 0.974,  Train: 69.956%,  Valid: 69.217%,  Test: 68.965%,  \n",
            "Epoch: 66, Loss: 0.974,  Train: 70.259%,  Valid: 69.472%,  Test: 68.874%,  \n",
            "Epoch: 67, Loss: 0.973,  Train: 70.339%,  Valid: 69.140%,  Test: 68.584%,  \n",
            "Epoch: 68, Loss: 0.967,  Train: 70.320%,  Valid: 69.291%,  Test: 68.881%,  \n",
            "Epoch: 69, Loss: 0.964,  Train: 69.994%,  Valid: 68.996%,  Test: 69.158%,  \n",
            "Epoch: 70, Loss: 0.965,  Train: 70.070%,  Valid: 68.939%,  Test: 68.920%,  \n",
            "Epoch: 71, Loss: 0.962,  Train: 70.363%,  Valid: 69.405%,  Test: 69.041%,  \n",
            "Epoch: 72, Loss: 0.958,  Train: 70.321%,  Valid: 69.126%,  Test: 68.953%,  \n",
            "Epoch: 73, Loss: 0.954,  Train: 70.326%,  Valid: 69.647%,  Test: 69.230%,  \n",
            "Epoch: 74, Loss: 0.954,  Train: 70.244%,  Valid: 69.204%,  Test: 69.140%,  \n",
            "Epoch: 75, Loss: 0.949,  Train: 70.145%,  Valid: 69.257%,  Test: 69.095%,  \n",
            "Epoch: 76, Loss: 0.947,  Train: 70.286%,  Valid: 69.113%,  Test: 69.109%,  \n",
            "Epoch: 77, Loss: 0.945,  Train: 70.395%,  Valid: 69.046%,  Test: 68.839%,  \n",
            "Epoch: 78, Loss: 0.943,  Train: 70.566%,  Valid: 69.247%,  Test: 69.045%,  \n",
            "Epoch: 79, Loss: 0.941,  Train: 70.505%,  Valid: 69.076%,  Test: 69.041%,  \n",
            "Epoch: 80, Loss: 0.939,  Train: 70.616%,  Valid: 69.217%,  Test: 69.261%,  \n",
            "Epoch: 81, Loss: 0.936,  Train: 70.662%,  Valid: 69.301%,  Test: 69.134%,  \n",
            "Epoch: 82, Loss: 0.935,  Train: 70.743%,  Valid: 69.126%,  Test: 69.064%,  \n",
            "Epoch: 83, Loss: 0.931,  Train: 70.685%,  Valid: 69.116%,  Test: 69.006%,  \n",
            "Epoch: 84, Loss: 0.928,  Train: 70.826%,  Valid: 68.992%,  Test: 69.267%,  \n",
            "Epoch: 85, Loss: 0.926,  Train: 70.756%,  Valid: 69.254%,  Test: 69.123%,  \n",
            "Epoch: 86, Loss: 0.929,  Train: 70.975%,  Valid: 69.583%,  Test: 69.222%,  \n",
            "Epoch: 87, Loss: 0.922,  Train: 70.977%,  Valid: 69.482%,  Test: 69.156%,  \n",
            "Epoch: 88, Loss: 0.920,  Train: 71.014%,  Valid: 69.509%,  Test: 69.175%,  \n",
            "Epoch: 89, Loss: 0.917,  Train: 71.173%,  Valid: 69.368%,  Test: 69.278%,  \n",
            "Epoch: 90, Loss: 0.917,  Train: 71.104%,  Valid: 69.690%,  Test: 69.290%,  \n",
            "Epoch: 91, Loss: 0.917,  Train: 71.203%,  Valid: 69.798%,  Test: 69.259%,  \n",
            "Epoch: 92, Loss: 0.913,  Train: 71.251%,  Valid: 69.630%,  Test: 69.329%,  \n",
            "Epoch: 93, Loss: 0.909,  Train: 71.424%,  Valid: 69.630%,  Test: 69.220%,  \n",
            "Epoch: 94, Loss: 0.908,  Train: 71.188%,  Valid: 69.439%,  Test: 69.020%,  \n",
            "Epoch: 95, Loss: 0.906,  Train: 71.179%,  Valid: 69.267%,  Test: 69.420%,  \n",
            "Epoch: 96, Loss: 0.902,  Train: 71.538%,  Valid: 69.965%,  Test: 69.492%,  \n",
            "Epoch: 97, Loss: 0.904,  Train: 71.648%,  Valid: 69.824%,  Test: 69.405%,  \n",
            "Epoch: 98, Loss: 0.899,  Train: 71.732%,  Valid: 70.143%,  Test: 69.313%,  \n",
            "Epoch: 99, Loss: 0.898,  Train: 71.830%,  Valid: 69.972%,  Test: 69.459%,  \n",
            "Epoch: 100, Loss: 0.897,  Train: 71.703%,  Valid: 69.915%,  Test: 69.592%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxMTkyK2y-Zb",
        "outputId": "863565f5-c089-40b7-b25a-548cd67a08e9"
      },
      "source": [
        "print_model_accuracy(best_sage, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.93%, Valid: 70.39% Test: 69.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksmxQNW_wuyv"
      },
      "source": [
        "### With normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUEmWo_KwfQB",
        "outputId": "fbab740a-307f-4b09-cf19-1cf778bfc168"
      },
      "source": [
        "sagenorm = SAGE(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "            output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "            dropout=args[\"dropout\"], normalize=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(sagenorm.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sagenorm, sagenorm_best_acc = train_loop(sagenorm.to(device), \n",
        "                                              data, optimizer, nll_loss, \n",
        "                                              args[\"epochs\"], split_idx, \n",
        "                                              dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.688,  Train: 24.851%,  Valid: 22.222%,  Test: 25.470%,  \n",
            "Epoch: 02, Loss: 3.432,  Train: 29.644%,  Valid: 32.340%,  Test: 36.185%,  \n",
            "Epoch: 03, Loss: 3.390,  Train: 30.794%,  Valid: 36.109%,  Test: 39.921%,  \n",
            "Epoch: 04, Loss: 3.353,  Train: 30.813%,  Valid: 37.807%,  Test: 41.164%,  \n",
            "Epoch: 05, Loss: 3.315,  Train: 31.129%,  Valid: 38.867%,  Test: 41.777%,  \n",
            "Epoch: 06, Loss: 3.282,  Train: 31.043%,  Valid: 38.521%,  Test: 40.856%,  \n",
            "Epoch: 07, Loss: 3.259,  Train: 30.323%,  Valid: 37.273%,  Test: 38.382%,  \n",
            "Epoch: 08, Loss: 3.243,  Train: 29.462%,  Valid: 35.719%,  Test: 36.385%,  \n",
            "Epoch: 09, Loss: 3.230,  Train: 28.435%,  Valid: 34.800%,  Test: 35.018%,  \n",
            "Epoch: 10, Loss: 3.216,  Train: 28.181%,  Valid: 35.233%,  Test: 35.586%,  \n",
            "Epoch: 11, Loss: 3.201,  Train: 28.875%,  Valid: 36.273%,  Test: 37.269%,  \n",
            "Epoch: 12, Loss: 3.187,  Train: 30.389%,  Valid: 39.001%,  Test: 40.759%,  \n",
            "Epoch: 13, Loss: 3.174,  Train: 32.539%,  Valid: 41.303%,  Test: 43.954%,  \n",
            "Epoch: 14, Loss: 3.161,  Train: 33.860%,  Valid: 42.387%,  Test: 45.865%,  \n",
            "Epoch: 15, Loss: 3.151,  Train: 34.388%,  Valid: 42.236%,  Test: 45.870%,  \n",
            "Epoch: 16, Loss: 3.143,  Train: 34.442%,  Valid: 41.511%,  Test: 45.326%,  \n",
            "Epoch: 17, Loss: 3.136,  Train: 33.794%,  Valid: 39.968%,  Test: 44.150%,  \n",
            "Epoch: 18, Loss: 3.129,  Train: 33.099%,  Valid: 38.548%,  Test: 43.380%,  \n",
            "Epoch: 19, Loss: 3.124,  Train: 33.512%,  Valid: 38.149%,  Test: 42.683%,  \n",
            "Epoch: 20, Loss: 3.119,  Train: 33.554%,  Valid: 38.186%,  Test: 42.518%,  \n",
            "Epoch: 21, Loss: 3.114,  Train: 34.138%,  Valid: 38.337%,  Test: 42.606%,  \n",
            "Epoch: 22, Loss: 3.110,  Train: 35.179%,  Valid: 39.709%,  Test: 43.713%,  \n",
            "Epoch: 23, Loss: 3.106,  Train: 36.291%,  Valid: 40.327%,  Test: 44.450%,  \n",
            "Epoch: 24, Loss: 3.102,  Train: 37.516%,  Valid: 41.679%,  Test: 45.863%,  \n",
            "Epoch: 25, Loss: 3.099,  Train: 39.015%,  Valid: 43.340%,  Test: 47.116%,  \n",
            "Epoch: 26, Loss: 3.095,  Train: 40.565%,  Valid: 45.565%,  Test: 48.664%,  \n",
            "Epoch: 27, Loss: 3.091,  Train: 42.282%,  Valid: 47.283%,  Test: 50.303%,  \n",
            "Epoch: 28, Loss: 3.088,  Train: 43.579%,  Valid: 49.109%,  Test: 51.847%,  \n",
            "Epoch: 29, Loss: 3.085,  Train: 45.167%,  Valid: 50.676%,  Test: 53.305%,  \n",
            "Epoch: 30, Loss: 3.081,  Train: 46.549%,  Valid: 51.767%,  Test: 54.019%,  \n",
            "Epoch: 31, Loss: 3.078,  Train: 47.411%,  Valid: 52.680%,  Test: 55.019%,  \n",
            "Epoch: 32, Loss: 3.075,  Train: 48.608%,  Valid: 53.371%,  Test: 55.239%,  \n",
            "Epoch: 33, Loss: 3.072,  Train: 49.814%,  Valid: 54.066%,  Test: 55.793%,  \n",
            "Epoch: 34, Loss: 3.069,  Train: 50.896%,  Valid: 54.821%,  Test: 56.420%,  \n",
            "Epoch: 35, Loss: 3.067,  Train: 52.150%,  Valid: 55.592%,  Test: 56.704%,  \n",
            "Epoch: 36, Loss: 3.064,  Train: 53.552%,  Valid: 56.572%,  Test: 57.379%,  \n",
            "Epoch: 37, Loss: 3.062,  Train: 54.490%,  Valid: 57.237%,  Test: 57.416%,  \n",
            "Epoch: 38, Loss: 3.060,  Train: 55.638%,  Valid: 57.878%,  Test: 57.961%,  \n",
            "Epoch: 39, Loss: 3.058,  Train: 56.055%,  Valid: 57.911%,  Test: 57.896%,  \n",
            "Epoch: 40, Loss: 3.056,  Train: 56.676%,  Valid: 58.210%,  Test: 57.916%,  \n",
            "Epoch: 41, Loss: 3.054,  Train: 57.199%,  Valid: 58.183%,  Test: 57.867%,  \n",
            "Epoch: 42, Loss: 3.053,  Train: 57.409%,  Valid: 58.425%,  Test: 57.809%,  \n",
            "Epoch: 43, Loss: 3.051,  Train: 57.524%,  Valid: 58.415%,  Test: 57.766%,  \n",
            "Epoch: 44, Loss: 3.049,  Train: 58.103%,  Valid: 58.817%,  Test: 58.081%,  \n",
            "Epoch: 45, Loss: 3.047,  Train: 58.691%,  Valid: 59.515%,  Test: 58.618%,  \n",
            "Epoch: 46, Loss: 3.045,  Train: 59.484%,  Valid: 60.106%,  Test: 59.396%,  \n",
            "Epoch: 47, Loss: 3.044,  Train: 60.219%,  Valid: 60.596%,  Test: 60.077%,  \n",
            "Epoch: 48, Loss: 3.042,  Train: 60.453%,  Valid: 60.985%,  Test: 60.352%,  \n",
            "Epoch: 49, Loss: 3.041,  Train: 60.880%,  Valid: 61.140%,  Test: 60.507%,  \n",
            "Epoch: 50, Loss: 3.039,  Train: 60.980%,  Valid: 61.502%,  Test: 60.589%,  \n",
            "Epoch: 51, Loss: 3.038,  Train: 61.333%,  Valid: 61.314%,  Test: 60.529%,  \n",
            "Epoch: 52, Loss: 3.035,  Train: 61.893%,  Valid: 61.918%,  Test: 61.280%,  \n",
            "Epoch: 53, Loss: 3.035,  Train: 62.501%,  Valid: 62.609%,  Test: 61.745%,  \n",
            "Epoch: 54, Loss: 3.033,  Train: 63.093%,  Valid: 63.277%,  Test: 62.632%,  \n",
            "Epoch: 55, Loss: 3.032,  Train: 63.549%,  Valid: 63.630%,  Test: 63.169%,  \n",
            "Epoch: 56, Loss: 3.031,  Train: 63.761%,  Valid: 63.878%,  Test: 63.570%,  \n",
            "Epoch: 57, Loss: 3.029,  Train: 64.210%,  Valid: 64.220%,  Test: 63.743%,  \n",
            "Epoch: 58, Loss: 3.028,  Train: 64.530%,  Valid: 64.355%,  Test: 63.938%,  \n",
            "Epoch: 59, Loss: 3.026,  Train: 64.767%,  Valid: 64.633%,  Test: 64.356%,  \n",
            "Epoch: 60, Loss: 3.025,  Train: 65.135%,  Valid: 65.395%,  Test: 64.517%,  \n",
            "Epoch: 61, Loss: 3.024,  Train: 65.354%,  Valid: 65.204%,  Test: 64.780%,  \n",
            "Epoch: 62, Loss: 3.022,  Train: 65.514%,  Valid: 65.452%,  Test: 64.885%,  \n",
            "Epoch: 63, Loss: 3.022,  Train: 65.743%,  Valid: 65.536%,  Test: 64.975%,  \n",
            "Epoch: 64, Loss: 3.020,  Train: 66.047%,  Valid: 65.844%,  Test: 65.224%,  \n",
            "Epoch: 65, Loss: 3.019,  Train: 66.416%,  Valid: 66.120%,  Test: 65.366%,  \n",
            "Epoch: 66, Loss: 3.018,  Train: 66.729%,  Valid: 66.455%,  Test: 65.473%,  \n",
            "Epoch: 67, Loss: 3.016,  Train: 66.747%,  Valid: 66.509%,  Test: 65.440%,  \n",
            "Epoch: 68, Loss: 3.016,  Train: 66.884%,  Valid: 66.858%,  Test: 65.603%,  \n",
            "Epoch: 69, Loss: 3.015,  Train: 67.064%,  Valid: 66.714%,  Test: 65.982%,  \n",
            "Epoch: 70, Loss: 3.013,  Train: 67.314%,  Valid: 66.660%,  Test: 65.860%,  \n",
            "Epoch: 71, Loss: 3.011,  Train: 67.580%,  Valid: 66.965%,  Test: 66.144%,  \n",
            "Epoch: 72, Loss: 3.011,  Train: 67.652%,  Valid: 67.274%,  Test: 66.393%,  \n",
            "Epoch: 73, Loss: 3.010,  Train: 67.751%,  Valid: 67.331%,  Test: 66.500%,  \n",
            "Epoch: 74, Loss: 3.008,  Train: 67.951%,  Valid: 67.412%,  Test: 66.687%,  \n",
            "Epoch: 75, Loss: 3.007,  Train: 68.389%,  Valid: 67.784%,  Test: 66.895%,  \n",
            "Epoch: 76, Loss: 3.006,  Train: 68.279%,  Valid: 67.747%,  Test: 66.932%,  \n",
            "Epoch: 77, Loss: 3.005,  Train: 68.397%,  Valid: 67.996%,  Test: 67.397%,  \n",
            "Epoch: 78, Loss: 3.004,  Train: 68.518%,  Valid: 68.009%,  Test: 67.235%,  \n",
            "Epoch: 79, Loss: 3.003,  Train: 68.739%,  Valid: 68.009%,  Test: 67.292%,  \n",
            "Epoch: 80, Loss: 3.002,  Train: 68.842%,  Valid: 68.197%,  Test: 67.681%,  \n",
            "Epoch: 81, Loss: 3.001,  Train: 68.888%,  Valid: 68.096%,  Test: 67.551%,  \n",
            "Epoch: 82, Loss: 3.000,  Train: 69.133%,  Valid: 68.338%,  Test: 67.671%,  \n",
            "Epoch: 83, Loss: 2.999,  Train: 69.120%,  Valid: 68.358%,  Test: 67.615%,  \n",
            "Epoch: 84, Loss: 2.998,  Train: 69.211%,  Valid: 68.123%,  Test: 67.648%,  \n",
            "Epoch: 85, Loss: 2.997,  Train: 69.271%,  Valid: 67.959%,  Test: 67.640%,  \n",
            "Epoch: 86, Loss: 2.996,  Train: 69.090%,  Valid: 68.341%,  Test: 67.648%,  \n",
            "Epoch: 87, Loss: 2.995,  Train: 69.248%,  Valid: 68.274%,  Test: 67.601%,  \n",
            "Epoch: 88, Loss: 2.993,  Train: 69.320%,  Valid: 68.418%,  Test: 67.662%,  \n",
            "Epoch: 89, Loss: 2.993,  Train: 69.525%,  Valid: 68.328%,  Test: 67.502%,  \n",
            "Epoch: 90, Loss: 2.992,  Train: 69.463%,  Valid: 68.462%,  Test: 67.609%,  \n",
            "Epoch: 91, Loss: 2.991,  Train: 69.511%,  Valid: 68.462%,  Test: 67.874%,  \n",
            "Epoch: 92, Loss: 2.990,  Train: 69.553%,  Valid: 68.241%,  Test: 67.821%,  \n",
            "Epoch: 93, Loss: 2.990,  Train: 69.655%,  Valid: 68.304%,  Test: 67.831%,  \n",
            "Epoch: 94, Loss: 2.989,  Train: 69.632%,  Valid: 68.462%,  Test: 68.027%,  \n",
            "Epoch: 95, Loss: 2.987,  Train: 69.684%,  Valid: 68.502%,  Test: 68.014%,  \n",
            "Epoch: 96, Loss: 2.986,  Train: 69.819%,  Valid: 68.663%,  Test: 67.901%,  \n",
            "Epoch: 97, Loss: 2.985,  Train: 69.996%,  Valid: 68.345%,  Test: 67.695%,  \n",
            "Epoch: 98, Loss: 2.985,  Train: 70.184%,  Valid: 68.422%,  Test: 67.702%,  \n",
            "Epoch: 99, Loss: 2.984,  Train: 70.235%,  Valid: 68.479%,  Test: 68.234%,  \n",
            "Epoch: 100, Loss: 2.983,  Train: 70.329%,  Valid: 68.717%,  Test: 68.292%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSIx_dZAzjLG",
        "outputId": "e1c1b6d1-9663-4169-e0c6-8f6335d71cd5"
      },
      "source": [
        "print_model_accuracy(best_sagenorm, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.93%, Valid: 70.13% Test: 68.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoETpGZQvPQ1"
      },
      "source": [
        "# GAT\n",
        "Scales badly need to decrease hidden_dim, otherwise cuda goes out of memory.\n",
        "See this: https://github.com/pyg-team/pytorch_geometric/issues/527"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd_zrnm3ujHe"
      },
      "source": [
        "class GAT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads,\n",
        "                 num_layers, dropout):\n",
        "      \n",
        "      super(GAT, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [GATConv(input_dim, hidden_dim, heads, concat=False)] +\n",
        "          [GATConv(hidden_dim, hidden_dim, heads, concat=False) for _ in range(num_layers - 2)]+\n",
        "          [GATConv(hidden_dim, output_dim, heads, concat=False)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "TLFdIsMwvpep",
        "outputId": "83d896a1-93d6-4aed-c4ad-654347dc1dc2"
      },
      "source": [
        "gat = GAT(input_dim=args[\"input_dim\"], hidden_dim=16,#args[\"hidden_dim\"], \n",
        "          output_dim=args[\"output_dim\"], heads=2,#args[\"heads\"],\n",
        "          num_layers=args[\"num_layers\"], dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(gat.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_gat, gat_best_acc = train_loop(gat.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8fa24b1ebbef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                       dataset_evaluator)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-2476ca512908>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, data, optimizer, loss_fn, epochs, split_idx, evaluator)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-48f1c4f14002>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, loss_fn, optimizer, train_idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-63c979b96503>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj_t)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor, alpha: OptPairTensor, edge_attr: OptTensor)  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         out = self.propagate(edge_index, x=x, alpha=alpha, edge_attr=edge_attr,\n\u001b[0;32m--> 219\u001b[0;31m                              size=size)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                         \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_j, alpha_j, alpha_i, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m  \u001b[0;31m# Save for later use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 11.17 GiB total capacity; 10.48 GiB already allocated; 22.81 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_GZGfAn7_Xe"
      },
      "source": [
        "## Stacking multiple layers together\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4r5iZKav-A3"
      },
      "source": [
        "from torch_geometric.utils import to_networkx, add_self_loops\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJzeX0_P8Mru",
        "outputId": "e17b0ae8-09e6-4146-d0c5-7d4180b0403f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Too slow\n",
        "#net = nx.from_scipy_sparse_matrix(data.adj_t.to_scipy())\n",
        "#d = nx.algorithms.distance_measures.diameter(net)\n",
        "#print(f\"Graph diameter is {d}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_sparse/storage.py:14: UserWarning: `layout` argument unset, using default layout \"coo\". This may lead to unexpected behaviour.\n",
            "  warnings.warn('`layout` argument unset, using default layout '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCeE7t39MjVb",
        "outputId": "c952c89f-3777-40b2-937f-3eb369b3c39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "todata.adj_t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([     0,      0,      0,  ..., 169341, 169342, 169342], device='cuda:0'),\n",
              "             col=tensor([   411,    640,   1162,  ..., 163274,  27824, 158981], device='cuda:0'),\n",
              "             size=(169343, 169343), nnz=2315598, density=0.01%)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMxXxDmgCuvA"
      },
      "source": [
        "## Stacking multiple linear layers\n",
        "* Message passing composed of n linear layers instead of one\n",
        "* Aggregation can be a mlp\n",
        "\n",
        "**Question** in aggregation how do we mantain the order invariant property?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJkineSJENov"
      },
      "source": [
        "import itertools\n",
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.utils import add_self_loops, degree, contains_self_loops\n",
        "from torch_geometric.nn.inits import zeros\n",
        "\n",
        "\n",
        "class DeepGCNConv(GCNConv):\n",
        "    \n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 improved: bool = False, cached: bool = False,\n",
        "                 add_self_loops: bool = True, normalize: bool = True,\n",
        "                 bias: bool = True, num_msg_layers: int = 1, \n",
        "                 num_agg_layers: int = 0,\n",
        "                 **kwargs):\n",
        "      \n",
        "        super().__init__(in_channels, out_channels, improved, cached, \n",
        "                         add_self_loops, normalize, bias, **kwargs)  \n",
        "\n",
        "        self.num_msg_layers = num_msg_layers\n",
        "        self.num_agg_layers = num_agg_layers\n",
        "\n",
        "        if self.num_msg_layers > 1:\n",
        "          mlp_start = [torch.nn.Linear(in_channels, out_channels), torch.nn.ReLU()]\n",
        "          mlp = list(\n",
        "              itertools.chain.from_iterable(\n",
        "              [[torch.nn.Linear(out_channels, out_channels), torch.nn.ReLU()]\n",
        "              for i in range(self.num_msg_layers - 2)])\n",
        "              )\n",
        "          mlp_end = [torch.nn.Linear(out_channels, out_channels)]\n",
        "          self.lin = torch.nn.Sequential(*(mlp_start + mlp + mlp_end))\n",
        "\n",
        "        if self.num_agg_layers == 1:\n",
        "          self.lin_agg = torch.nn.Linear(out_channels, out_channels)\n",
        "        elif self.num_agg_layers > 1:\n",
        "          mlp_start = [torch.nn.Linear(out_channels, out_channels), torch.nn.ReLU()]\n",
        "          mlp = list(\n",
        "              itertools.chain.from_iterable(\n",
        "              [[torch.nn.Linear(out_channels, out_channels), torch.nn.ReLU()]\n",
        "              for i in range(self.num_agg_layers - 2)])\n",
        "              )\n",
        "          mlp_end = [torch.nn.Linear(out_channels, out_channels)]\n",
        "          self.lin_agg = torch.nn.Sequential(*(mlp_start + mlp + mlp_end))\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        if self.num_agg_layers > 0:\n",
        "          out = matmul(adj_t, x, reduce=self.aggr)\n",
        "          return self.lin_agg(out)\n",
        "        else:\n",
        "          return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        #self.lin.reset_parameters()\n",
        "        zeros(self.bias)\n",
        "        self._cached_edge_index = None\n",
        "        self._cached_adj_t = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUtz-4RN-QvT"
      },
      "source": [
        "class DeepGCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, \n",
        "                 num_msg_layers, num_agg_layers, num_layers, dropout):\n",
        "      \n",
        "      super(DeepGCN, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [DeepGCNConv(input_dim, hidden_dim, num_msg_layers=num_msg_layers,\n",
        "                       num_agg_layers=num_agg_layers)] +\n",
        "          [DeepGCNConv(hidden_dim, hidden_dim, num_msg_layers=num_msg_layers, \n",
        "                       num_agg_layers=num_agg_layers) \n",
        "           for _ in range(num_layers - 2)]+\n",
        "          [DeepGCNConv(hidden_dim, output_dim, num_msg_layers=num_msg_layers, \n",
        "                       num_agg_layers=num_agg_layers)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 layer mlp in message passing"
      ],
      "metadata": {
        "id": "fTLexjg3JGil"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09vb8v3g8X09",
        "outputId": "c8b999ee-5a37-4918-b8f8-2412234958f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dgcn = DeepGCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"],\n",
        "               output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"],\n",
        "               num_msg_layers=3, num_agg_layers=0, dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(dgcn.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_dgcn, dgcn_best_acc = train_loop(dgcn.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.736,  Train: 10.994%,  Valid: 22.974%,  Test: 21.556%,  \n",
            "Epoch: 02, Loss: 3.632,  Train: 10.994%,  Valid: 22.974%,  Test: 21.556%,  \n",
            "Epoch: 03, Loss: 3.480,  Train: 10.994%,  Valid: 22.974%,  Test: 21.556%,  \n",
            "Epoch: 04, Loss: 3.253,  Train: 10.994%,  Valid: 22.974%,  Test: 21.556%,  \n",
            "Epoch: 05, Loss: 3.064,  Train: 25.629%,  Valid: 28.652%,  Test: 25.745%,  \n",
            "Epoch: 06, Loss: 2.821,  Train: 17.908%,  Valid: 7.628%,  Test: 5.866%,  \n",
            "Epoch: 07, Loss: 2.689,  Train: 17.906%,  Valid: 7.628%,  Test: 5.862%,  \n",
            "Epoch: 08, Loss: 2.702,  Train: 17.918%,  Valid: 7.634%,  Test: 5.872%,  \n",
            "Epoch: 09, Loss: 2.625,  Train: 23.215%,  Valid: 23.286%,  Test: 22.139%,  \n",
            "Epoch: 10, Loss: 2.580,  Train: 30.143%,  Valid: 34.652%,  Test: 31.700%,  \n",
            "Epoch: 11, Loss: 2.553,  Train: 30.081%,  Valid: 33.810%,  Test: 30.171%,  \n",
            "Epoch: 12, Loss: 2.511,  Train: 29.869%,  Valid: 32.904%,  Test: 29.811%,  \n",
            "Epoch: 13, Loss: 2.490,  Train: 30.515%,  Valid: 32.421%,  Test: 29.904%,  \n",
            "Epoch: 14, Loss: 2.468,  Train: 31.283%,  Valid: 32.290%,  Test: 29.519%,  \n",
            "Epoch: 15, Loss: 2.435,  Train: 30.425%,  Valid: 31.709%,  Test: 28.474%,  \n",
            "Epoch: 16, Loss: 2.409,  Train: 30.931%,  Valid: 32.145%,  Test: 28.862%,  \n",
            "Epoch: 17, Loss: 2.390,  Train: 29.290%,  Valid: 31.820%,  Test: 28.891%,  \n",
            "Epoch: 18, Loss: 2.356,  Train: 25.854%,  Valid: 28.034%,  Test: 26.210%,  \n",
            "Epoch: 19, Loss: 2.327,  Train: 25.403%,  Valid: 26.776%,  Test: 25.698%,  \n",
            "Epoch: 20, Loss: 2.301,  Train: 30.551%,  Valid: 33.367%,  Test: 32.144%,  \n",
            "Epoch: 21, Loss: 2.274,  Train: 33.878%,  Valid: 34.095%,  Test: 30.529%,  \n",
            "Epoch: 22, Loss: 2.249,  Train: 33.548%,  Valid: 33.874%,  Test: 30.562%,  \n",
            "Epoch: 23, Loss: 2.221,  Train: 32.295%,  Valid: 33.447%,  Test: 30.344%,  \n",
            "Epoch: 24, Loss: 2.189,  Train: 33.451%,  Valid: 34.632%,  Test: 31.512%,  \n",
            "Epoch: 25, Loss: 2.161,  Train: 36.368%,  Valid: 37.350%,  Test: 33.944%,  \n",
            "Epoch: 26, Loss: 2.126,  Train: 35.956%,  Valid: 40.079%,  Test: 36.917%,  \n",
            "Epoch: 27, Loss: 2.094,  Train: 32.196%,  Valid: 38.414%,  Test: 38.154%,  \n",
            "Epoch: 28, Loss: 2.072,  Train: 36.234%,  Valid: 41.542%,  Test: 38.687%,  \n",
            "Epoch: 29, Loss: 2.050,  Train: 38.754%,  Valid: 43.710%,  Test: 41.259%,  \n",
            "Epoch: 30, Loss: 2.023,  Train: 40.355%,  Valid: 41.542%,  Test: 37.125%,  \n",
            "Epoch: 31, Loss: 1.988,  Train: 41.333%,  Valid: 41.438%,  Test: 37.234%,  \n",
            "Epoch: 32, Loss: 1.967,  Train: 43.985%,  Valid: 45.639%,  Test: 42.238%,  \n",
            "Epoch: 33, Loss: 1.950,  Train: 39.090%,  Valid: 39.723%,  Test: 35.827%,  \n",
            "Epoch: 34, Loss: 1.944,  Train: 43.041%,  Valid: 45.592%,  Test: 42.417%,  \n",
            "Epoch: 35, Loss: 1.906,  Train: 41.495%,  Valid: 44.233%,  Test: 40.954%,  \n",
            "Epoch: 36, Loss: 1.889,  Train: 39.937%,  Valid: 41.652%,  Test: 37.847%,  \n",
            "Epoch: 37, Loss: 1.891,  Train: 42.498%,  Valid: 46.683%,  Test: 43.318%,  \n",
            "Epoch: 38, Loss: 1.883,  Train: 42.945%,  Valid: 45.666%,  Test: 42.543%,  \n",
            "Epoch: 39, Loss: 1.861,  Train: 41.689%,  Valid: 45.481%,  Test: 42.181%,  \n",
            "Epoch: 40, Loss: 1.846,  Train: 41.016%,  Valid: 46.421%,  Test: 43.265%,  \n",
            "Epoch: 41, Loss: 1.837,  Train: 41.020%,  Valid: 45.334%,  Test: 42.150%,  \n",
            "Epoch: 42, Loss: 1.827,  Train: 41.293%,  Valid: 45.032%,  Test: 41.703%,  \n",
            "Epoch: 43, Loss: 1.804,  Train: 40.576%,  Valid: 45.206%,  Test: 42.205%,  \n",
            "Epoch: 44, Loss: 1.794,  Train: 39.137%,  Valid: 44.149%,  Test: 41.061%,  \n",
            "Epoch: 45, Loss: 1.783,  Train: 39.634%,  Valid: 44.548%,  Test: 41.662%,  \n",
            "Epoch: 46, Loss: 1.769,  Train: 41.838%,  Valid: 46.015%,  Test: 42.993%,  \n",
            "Epoch: 47, Loss: 1.755,  Train: 41.502%,  Valid: 46.062%,  Test: 43.327%,  \n",
            "Epoch: 48, Loss: 1.743,  Train: 39.516%,  Valid: 43.793%,  Test: 41.598%,  \n",
            "Epoch: 49, Loss: 1.739,  Train: 44.053%,  Valid: 47.505%,  Test: 44.557%,  \n",
            "Epoch: 50, Loss: 1.722,  Train: 47.168%,  Valid: 49.971%,  Test: 47.178%,  \n",
            "Epoch: 51, Loss: 1.716,  Train: 43.220%,  Valid: 46.632%,  Test: 42.985%,  \n",
            "Epoch: 52, Loss: 1.706,  Train: 41.654%,  Valid: 45.582%,  Test: 42.384%,  \n",
            "Epoch: 53, Loss: 1.693,  Train: 41.865%,  Valid: 42.958%,  Test: 39.627%,  \n",
            "Epoch: 54, Loss: 1.683,  Train: 42.382%,  Valid: 44.045%,  Test: 40.677%,  \n",
            "Epoch: 55, Loss: 1.680,  Train: 42.242%,  Valid: 43.659%,  Test: 39.993%,  \n",
            "Epoch: 56, Loss: 1.662,  Train: 40.249%,  Valid: 41.300%,  Test: 37.716%,  \n",
            "Epoch: 57, Loss: 1.656,  Train: 40.967%,  Valid: 42.703%,  Test: 38.961%,  \n",
            "Epoch: 58, Loss: 1.645,  Train: 42.036%,  Valid: 42.820%,  Test: 38.852%,  \n",
            "Epoch: 59, Loss: 1.633,  Train: 41.052%,  Valid: 42.961%,  Test: 39.214%,  \n",
            "Epoch: 60, Loss: 1.633,  Train: 42.002%,  Valid: 41.723%,  Test: 37.928%,  \n",
            "Epoch: 61, Loss: 1.641,  Train: 42.403%,  Valid: 46.585%,  Test: 43.514%,  \n",
            "Epoch: 62, Loss: 1.667,  Train: 46.208%,  Valid: 43.840%,  Test: 39.504%,  \n",
            "Epoch: 63, Loss: 1.660,  Train: 50.114%,  Valid: 51.797%,  Test: 49.003%,  \n",
            "Epoch: 64, Loss: 1.606,  Train: 51.029%,  Valid: 54.223%,  Test: 53.521%,  \n",
            "Epoch: 65, Loss: 1.632,  Train: 53.904%,  Valid: 54.606%,  Test: 51.921%,  \n",
            "Epoch: 66, Loss: 1.593,  Train: 52.305%,  Valid: 53.126%,  Test: 50.118%,  \n",
            "Epoch: 67, Loss: 1.600,  Train: 52.927%,  Valid: 55.254%,  Test: 54.904%,  \n",
            "Epoch: 68, Loss: 1.577,  Train: 54.006%,  Valid: 54.965%,  Test: 56.381%,  \n",
            "Epoch: 69, Loss: 1.584,  Train: 55.305%,  Valid: 56.512%,  Test: 54.616%,  \n",
            "Epoch: 70, Loss: 1.564,  Train: 55.303%,  Valid: 55.562%,  Test: 52.896%,  \n",
            "Epoch: 71, Loss: 1.560,  Train: 55.237%,  Valid: 55.965%,  Test: 54.091%,  \n",
            "Epoch: 72, Loss: 1.542,  Train: 55.559%,  Valid: 57.082%,  Test: 56.017%,  \n",
            "Epoch: 73, Loss: 1.541,  Train: 55.517%,  Valid: 56.603%,  Test: 54.929%,  \n",
            "Epoch: 74, Loss: 1.528,  Train: 53.296%,  Valid: 54.381%,  Test: 53.167%,  \n",
            "Epoch: 75, Loss: 1.526,  Train: 53.194%,  Valid: 55.542%,  Test: 54.060%,  \n",
            "Epoch: 76, Loss: 1.510,  Train: 52.894%,  Valid: 55.723%,  Test: 55.032%,  \n",
            "Epoch: 77, Loss: 1.511,  Train: 52.602%,  Valid: 55.572%,  Test: 54.943%,  \n",
            "Epoch: 78, Loss: 1.492,  Train: 50.181%,  Valid: 53.247%,  Test: 53.879%,  \n",
            "Epoch: 79, Loss: 1.495,  Train: 51.687%,  Valid: 55.287%,  Test: 55.227%,  \n",
            "Epoch: 80, Loss: 1.479,  Train: 47.888%,  Valid: 51.512%,  Test: 52.608%,  \n",
            "Epoch: 81, Loss: 1.477,  Train: 46.923%,  Valid: 51.139%,  Test: 52.886%,  \n",
            "Epoch: 82, Loss: 1.462,  Train: 44.300%,  Valid: 46.830%,  Test: 49.822%,  \n",
            "Epoch: 83, Loss: 1.457,  Train: 42.914%,  Valid: 45.995%,  Test: 48.738%,  \n",
            "Epoch: 84, Loss: 1.449,  Train: 45.134%,  Valid: 49.998%,  Test: 52.145%,  \n",
            "Epoch: 85, Loss: 1.453,  Train: 37.543%,  Valid: 40.115%,  Test: 43.483%,  \n",
            "Epoch: 86, Loss: 1.472,  Train: 42.314%,  Valid: 42.787%,  Test: 45.592%,  \n",
            "Epoch: 87, Loss: 1.503,  Train: 47.546%,  Valid: 53.086%,  Test: 54.715%,  \n",
            "Epoch: 88, Loss: 1.479,  Train: 42.634%,  Valid: 43.089%,  Test: 46.512%,  \n",
            "Epoch: 89, Loss: 1.526,  Train: 44.447%,  Valid: 45.226%,  Test: 46.516%,  \n",
            "Epoch: 90, Loss: 1.455,  Train: 49.679%,  Valid: 49.820%,  Test: 49.009%,  \n",
            "Epoch: 91, Loss: 1.476,  Train: 49.657%,  Valid: 52.223%,  Test: 53.384%,  \n",
            "Epoch: 92, Loss: 1.443,  Train: 51.669%,  Valid: 53.609%,  Test: 55.898%,  \n",
            "Epoch: 93, Loss: 1.446,  Train: 54.242%,  Valid: 55.720%,  Test: 56.161%,  \n",
            "Epoch: 94, Loss: 1.437,  Train: 57.752%,  Valid: 58.787%,  Test: 56.894%,  \n",
            "Epoch: 95, Loss: 1.419,  Train: 58.221%,  Valid: 59.569%,  Test: 58.601%,  \n",
            "Epoch: 96, Loss: 1.416,  Train: 58.369%,  Valid: 60.136%,  Test: 60.782%,  \n",
            "Epoch: 97, Loss: 1.400,  Train: 56.699%,  Valid: 57.173%,  Test: 58.786%,  \n",
            "Epoch: 98, Loss: 1.399,  Train: 56.134%,  Valid: 56.492%,  Test: 57.587%,  \n",
            "Epoch: 99, Loss: 1.390,  Train: 57.543%,  Valid: 59.448%,  Test: 58.612%,  \n",
            "Epoch: 100, Loss: 1.379,  Train: 58.488%,  Valid: 60.814%,  Test: 60.895%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7vfAYeJGO1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9153139c-abde-4c4f-ce03-518d2fab1084"
      },
      "source": [
        "print_model_accuracy(best_dgcn, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 58.43%, Valid: 60.99% Test: 61.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 layer mlp aggregatio"
      ],
      "metadata": {
        "id": "5Yl2b8pDLM-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dgcna = DeepGCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"],\n",
        "               output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"],\n",
        "               num_msg_layers=1, num_agg_layers=2, dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(dgcna.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_dgcna, dgcna_best_acc = train_loop(dgcna.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "metadata": {
        "id": "sgK78sJvLIdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_accuracy(best_dgcna, data, split_idx, dataset_evaluator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7UA5o3YLTfa",
        "outputId": "1aabe52a-5d93-497e-c6a8-88617c3884a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 65.71%, Valid: 65.89% Test: 65.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del dgcna, dgcn"
      ],
      "metadata": {
        "id": "WvbAN8_BMhc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mwlGPgISMlfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dgcnma = DeepGCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"],\n",
        "               output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"],\n",
        "               num_msg_layers=2, num_agg_layers=2, dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(dgcnma.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_dgcnma, dgcnma_best_acc = train_loop(dgcnma.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "metadata": {
        "id": "8x4_-7MTMQjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_accuracy(best_dgcnma, data, split_idx, dataset_evaluator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaXQcUk4MaIF",
        "outputId": "b164a0e9-a5ae-4ed2-d60c-e521af644c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 51.50%, Valid: 54.47% Test: 53.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add layers that do not pass message\n"
      ],
      "metadata": {
        "id": "wlKZk4R4NkGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gcn2 = GCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"],\n",
        "               output_dim=args[\"output_dim\"], num_gcn_layers=args[\"num_layers\"],\n",
        "               num_preprocess_layers=2, num_postprocess_layers=2, dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(gcn2.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_gcn2, gcn2_best_acc = train_loop(gcn2.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "metadata": {
        "id": "2AK28CUaQCtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_accuracy(best_gcn2, data, split_idx, dataset_evaluator)"
      ],
      "metadata": {
        "id": "zmzpTPwHQu-P",
        "outputId": "c1423973-0f9f-457f-dd93-e2cbd0c07a2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 64.34%, Valid: 64.29% Test: 65.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip connections\n"
      ],
      "metadata": {
        "id": "Rf8aTS6sLY8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGCN(GCN):\n",
        "\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "      out = x\n",
        "      if self.num_preprocess_layers > 0:\n",
        "        out = self.preprocess(out)\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t) + out\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      out = self.convs[-1](out, adj_t) + out\n",
        "      if self.num_postprocess_layers > 0:\n",
        "        out = self.bns[-1](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.postprocess(out)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "9BmCFzvLQc4B"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgcn = SkipGCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"],\n",
        "               output_dim=args[\"output_dim\"], num_gcn_layers=args[\"num_layers\"],\n",
        "               num_preprocess_layers=2, num_postprocess_layers=2, \n",
        "               dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(sgcn.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sgcn, sgcn_best_acc = train_loop(sgcn.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "metadata": {
        "id": "BXl7XaRKQ6dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_accuracy(best_sgcn, data, split_idx, dataset_evaluator)"
      ],
      "metadata": {
        "id": "A4cXTekQRBus",
        "outputId": "d1ba868b-af6e-4487-cd8d-f4527c4eb060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 65.41%, Valid: 66.60% Test: 65.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip to the end"
      ],
      "metadata": {
        "id": "vPS2flF9RwhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipEndGCN(GCN):\n",
        "\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "      accumulate_gcn = 0.\n",
        "      out = x\n",
        "      if self.num_preprocess_layers > 0:\n",
        "        out = self.preprocess(out)\n",
        "        accumulate_gcn += out\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "        accumulate_gcn += out\n",
        "      out = self.convs[-1](out, adj_t) + accumulate_gcn\n",
        "      if self.num_postprocess_layers > 0:\n",
        "        out = self.bns[-1](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.postprocess(out)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "XjW_uGXYRsSN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgcn2 = SkipEndGCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"],\n",
        "               output_dim=args[\"output_dim\"], num_gcn_layers=args[\"num_layers\"],\n",
        "               num_preprocess_layers=2, num_postprocess_layers=2, \n",
        "               dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(sgcn2.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sgcn2, sgcn2_best_acc = train_loop(sgcn2.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "metadata": {
        "id": "9wkB3CpaSrWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_accuracy(best_sgcn2, data, split_idx, dataset_evaluator)"
      ],
      "metadata": {
        "id": "1Gt8Ppj4SvjN",
        "outputId": "2120c5df-edc1-47a4-8aa6-6a387ce8ed96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 64.90%, Valid: 64.83% Test: 63.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}