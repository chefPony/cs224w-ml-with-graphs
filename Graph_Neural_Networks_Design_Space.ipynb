{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Neural Networks: Design Space.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrhT9QkVtPNpYoR9hMKt9f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefPony/cs224w-ml-with-graphs/blob/main/Graph_Neural_Networks_Design_Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tZ_uWPluKeJ",
        "outputId": "03a6ae2f-f4b0-459b-e23a-21711365241a"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p983o-Uf3goM",
        "outputId": "a5fbea5d-37c8-4ba5-e2d9-ce6250c3ae36"
      },
      "source": [
        "# Install torch geometric\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.1.5)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh9CqxAr3rMf",
        "outputId": "e68f50d3-8f10-4b6f-ec59-aa999e10732c"
      },
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  # Load the dataset and transform it to sparse tensor\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "  # Extract the graph\n",
        "  data = dataset[0]\n",
        "  print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiP3NGiD38Z7",
        "outputId": "699c4fe1-d58a-4c5f-e2c2-5b700dac0ebe"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyVZ_4lw4MxH",
        "outputId": "6259540c-c8c7-46fb-856a-a83a78e439a3"
      },
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  data = dataset[0]\n",
        "\n",
        "  # Make the adjacency matrix to symmetric\n",
        "  data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  # If you use GPU, the device should be cuda\n",
        "  print('Device: {}'.format(device))\n",
        "\n",
        "  data = data.to(device)\n",
        "  split_idx = dataset.get_idx_split()\n",
        "  train_idx = split_idx['train'].to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBQZRx5t8KS3"
      },
      "source": [
        "def train(model, data, loss_fn, optimizer, train_idx):\n",
        "    \n",
        "  model.train()\n",
        "  loss = 0\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  y_pred = model(data.x, data.adj_t)\n",
        "  loss = loss_fn(y_pred[train_idx], data.y[train_idx].squeeze())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z-SUCMS83SU"
      },
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  y_pred = model(data.x, data.adj_t)\n",
        "  y_pred = y_pred.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "  train_acc = evaluator.eval({\n",
        "      \"y_pred\": y_pred[split_idx[\"train\"]],\n",
        "      \"y_true\": data.y[split_idx[\"train\"]]\n",
        "  })[\"acc\"]\n",
        "  val_acc = evaluator.eval({\n",
        "    \"y_pred\": y_pred[split_idx[\"valid\"]],\n",
        "    \"y_true\": data.y[split_idx[\"valid\"]]\n",
        "  })[\"acc\"]\n",
        "  test_acc = evaluator.eval({\n",
        "    \"y_pred\": y_pred[split_idx[\"test\"]],\n",
        "    \"y_true\": data.y[split_idx[\"test\"]]\n",
        "  })[\"acc\"]\n",
        "\n",
        "  return train_acc, val_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFgcuHT5-9Br"
      },
      "source": [
        "def train_loop(model, data, optimizer, loss_fn, epochs, split_idx, evaluator):\n",
        "\n",
        "  best_val_acc = 0.\n",
        "\n",
        "  model.reset_parameters()\n",
        "\n",
        "  for e in range(1, epochs+1):\n",
        "    loss = train(model, data, loss_fn, optimizer, split_idx[\"train\"])\n",
        "    train_acc, val_acc, test_acc = test(model, data, split_idx, evaluator)\n",
        "\n",
        "    if val_acc>best_val_acc:\n",
        "      best_model = copy.deepcopy(gcn)\n",
        "      best_val_acc = val_acc\n",
        "\n",
        "    print(f\"Epoch: {e:02d}, Loss: {loss:.3f},  \"\n",
        "        f\"Train: {100 * train_acc:.3f}%,  \"   \n",
        "        f\"Valid: {100 * val_acc:.3f}%,  \" \n",
        "        f\"Test: {100 * test_acc:.3f}%,  \"\n",
        "        )\n",
        "  return best_model, best_val_acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckQ_JxV6tv94"
      },
      "source": [
        "def print_model_accuracy(best_model, data, split_idx, evaluator):\n",
        "  best_result = test(best_model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzlvSjiuNFZ"
      },
      "source": [
        "dataset_evaluator = Evaluator(name=dataset_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Dwgasx-Np2"
      },
      "source": [
        "args = {\n",
        "    \"input_dim\": data.num_features,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"output_dim\": dataset.num_classes,\n",
        "    \"num_layers\": 3,\n",
        "    \"heads\": 2,\n",
        "    \"dropout\": 0.5,\n",
        "    \"epochs\": 100,\n",
        "    \"lr\": 0.01\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW40CRXupLuR"
      },
      "source": [
        "## Vanilla GCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92LyQhFB6Qrf"
      },
      "source": [
        "class GCN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
        "      \n",
        "      super(GCN, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [GCNConv(input_dim, hidden_dim)] +\n",
        "          [GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)]+\n",
        "          [GCNConv(hidden_dim, output_dim)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFyB_q-rqAE5",
        "outputId": "7e8ad40c-6fe2-44ae-9a6a-240e4713a6f5"
      },
      "source": [
        "gcn = GCN(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "          output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "          dropout=args[\"dropout\"])\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(gcn.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_gcn, gcn_best_acc = train_loop(gcn.to(device), \n",
        "                                    data, optimizer, nll_loss, \n",
        "                                    args[\"epochs\"], split_idx, \n",
        "                                    dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.053,  Train: 27.935%,  Valid: 30.189%,  Test: 27.239%,  \n",
            "Epoch: 02, Loss: 2.356,  Train: 22.725%,  Valid: 17.692%,  Test: 19.785%,  \n",
            "Epoch: 03, Loss: 1.952,  Train: 23.919%,  Valid: 18.212%,  Test: 19.793%,  \n",
            "Epoch: 04, Loss: 1.806,  Train: 35.512%,  Valid: 36.971%,  Test: 37.115%,  \n",
            "Epoch: 05, Loss: 1.681,  Train: 41.395%,  Valid: 39.897%,  Test: 37.959%,  \n",
            "Epoch: 06, Loss: 1.599,  Train: 41.083%,  Valid: 37.823%,  Test: 37.825%,  \n",
            "Epoch: 07, Loss: 1.525,  Train: 40.967%,  Valid: 38.991%,  Test: 42.467%,  \n",
            "Epoch: 08, Loss: 1.464,  Train: 40.069%,  Valid: 39.545%,  Test: 43.532%,  \n",
            "Epoch: 09, Loss: 1.420,  Train: 39.554%,  Valid: 39.223%,  Test: 42.995%,  \n",
            "Epoch: 10, Loss: 1.383,  Train: 39.566%,  Valid: 39.448%,  Test: 43.211%,  \n",
            "Epoch: 11, Loss: 1.346,  Train: 41.034%,  Valid: 42.941%,  Test: 46.252%,  \n",
            "Epoch: 12, Loss: 1.313,  Train: 40.034%,  Valid: 41.934%,  Test: 45.606%,  \n",
            "Epoch: 13, Loss: 1.295,  Train: 39.517%,  Valid: 42.599%,  Test: 45.516%,  \n",
            "Epoch: 14, Loss: 1.274,  Train: 38.784%,  Valid: 41.226%,  Test: 44.314%,  \n",
            "Epoch: 15, Loss: 1.257,  Train: 40.603%,  Valid: 42.834%,  Test: 45.248%,  \n",
            "Epoch: 16, Loss: 1.242,  Train: 41.938%,  Valid: 43.652%,  Test: 45.925%,  \n",
            "Epoch: 17, Loss: 1.227,  Train: 44.507%,  Valid: 46.257%,  Test: 48.641%,  \n",
            "Epoch: 18, Loss: 1.213,  Train: 46.400%,  Valid: 48.804%,  Test: 52.200%,  \n",
            "Epoch: 19, Loss: 1.202,  Train: 48.137%,  Valid: 51.770%,  Test: 55.260%,  \n",
            "Epoch: 20, Loss: 1.189,  Train: 49.151%,  Valid: 52.190%,  Test: 55.887%,  \n",
            "Epoch: 21, Loss: 1.177,  Train: 50.842%,  Valid: 53.253%,  Test: 56.089%,  \n",
            "Epoch: 22, Loss: 1.164,  Train: 52.699%,  Valid: 54.851%,  Test: 57.571%,  \n",
            "Epoch: 23, Loss: 1.155,  Train: 54.567%,  Valid: 56.707%,  Test: 59.278%,  \n",
            "Epoch: 24, Loss: 1.146,  Train: 55.959%,  Valid: 57.579%,  Test: 59.858%,  \n",
            "Epoch: 25, Loss: 1.137,  Train: 57.629%,  Valid: 59.203%,  Test: 61.105%,  \n",
            "Epoch: 26, Loss: 1.126,  Train: 58.042%,  Valid: 59.519%,  Test: 61.743%,  \n",
            "Epoch: 27, Loss: 1.120,  Train: 59.062%,  Valid: 60.750%,  Test: 62.749%,  \n",
            "Epoch: 28, Loss: 1.113,  Train: 60.488%,  Valid: 61.831%,  Test: 63.864%,  \n",
            "Epoch: 29, Loss: 1.106,  Train: 61.633%,  Valid: 62.767%,  Test: 64.278%,  \n",
            "Epoch: 30, Loss: 1.102,  Train: 63.210%,  Valid: 63.969%,  Test: 65.029%,  \n",
            "Epoch: 31, Loss: 1.093,  Train: 64.147%,  Valid: 64.506%,  Test: 65.586%,  \n",
            "Epoch: 32, Loss: 1.088,  Train: 64.349%,  Valid: 64.767%,  Test: 65.613%,  \n",
            "Epoch: 33, Loss: 1.087,  Train: 65.006%,  Valid: 65.036%,  Test: 66.008%,  \n",
            "Epoch: 34, Loss: 1.080,  Train: 65.425%,  Valid: 65.667%,  Test: 66.430%,  \n",
            "Epoch: 35, Loss: 1.074,  Train: 65.681%,  Valid: 65.972%,  Test: 66.210%,  \n",
            "Epoch: 36, Loss: 1.068,  Train: 65.979%,  Valid: 66.579%,  Test: 66.591%,  \n",
            "Epoch: 37, Loss: 1.062,  Train: 66.273%,  Valid: 66.459%,  Test: 66.940%,  \n",
            "Epoch: 38, Loss: 1.060,  Train: 66.077%,  Valid: 66.183%,  Test: 67.064%,  \n",
            "Epoch: 39, Loss: 1.053,  Train: 66.321%,  Valid: 66.563%,  Test: 67.278%,  \n",
            "Epoch: 40, Loss: 1.050,  Train: 67.058%,  Valid: 67.432%,  Test: 67.969%,  \n",
            "Epoch: 41, Loss: 1.049,  Train: 67.517%,  Valid: 67.845%,  Test: 67.977%,  \n",
            "Epoch: 42, Loss: 1.045,  Train: 67.836%,  Valid: 68.412%,  Test: 68.391%,  \n",
            "Epoch: 43, Loss: 1.038,  Train: 68.099%,  Valid: 68.496%,  Test: 68.385%,  \n",
            "Epoch: 44, Loss: 1.035,  Train: 68.452%,  Valid: 68.576%,  Test: 68.603%,  \n",
            "Epoch: 45, Loss: 1.029,  Train: 69.007%,  Valid: 68.845%,  Test: 68.346%,  \n",
            "Epoch: 46, Loss: 1.026,  Train: 69.049%,  Valid: 68.788%,  Test: 67.759%,  \n",
            "Epoch: 47, Loss: 1.023,  Train: 69.226%,  Valid: 68.358%,  Test: 66.907%,  \n",
            "Epoch: 48, Loss: 1.024,  Train: 69.096%,  Valid: 68.650%,  Test: 67.029%,  \n",
            "Epoch: 49, Loss: 1.019,  Train: 69.216%,  Valid: 69.053%,  Test: 68.607%,  \n",
            "Epoch: 50, Loss: 1.016,  Train: 69.188%,  Valid: 68.925%,  Test: 68.654%,  \n",
            "Epoch: 51, Loss: 1.010,  Train: 69.441%,  Valid: 69.408%,  Test: 68.430%,  \n",
            "Epoch: 52, Loss: 1.007,  Train: 69.486%,  Valid: 69.059%,  Test: 68.319%,  \n",
            "Epoch: 53, Loss: 1.003,  Train: 69.705%,  Valid: 69.160%,  Test: 67.852%,  \n",
            "Epoch: 54, Loss: 1.004,  Train: 69.525%,  Valid: 69.043%,  Test: 68.080%,  \n",
            "Epoch: 55, Loss: 1.002,  Train: 69.606%,  Valid: 69.173%,  Test: 68.510%,  \n",
            "Epoch: 56, Loss: 0.998,  Train: 69.702%,  Valid: 69.331%,  Test: 68.557%,  \n",
            "Epoch: 57, Loss: 0.994,  Train: 69.614%,  Valid: 69.257%,  Test: 68.691%,  \n",
            "Epoch: 58, Loss: 0.994,  Train: 69.846%,  Valid: 69.368%,  Test: 69.035%,  \n",
            "Epoch: 59, Loss: 0.988,  Train: 70.077%,  Valid: 69.650%,  Test: 68.702%,  \n",
            "Epoch: 60, Loss: 0.990,  Train: 70.222%,  Valid: 69.338%,  Test: 68.590%,  \n",
            "Epoch: 61, Loss: 0.985,  Train: 70.181%,  Valid: 69.798%,  Test: 69.016%,  \n",
            "Epoch: 62, Loss: 0.986,  Train: 70.172%,  Valid: 69.660%,  Test: 69.261%,  \n",
            "Epoch: 63, Loss: 0.980,  Train: 70.136%,  Valid: 69.848%,  Test: 69.389%,  \n",
            "Epoch: 64, Loss: 0.978,  Train: 70.414%,  Valid: 69.855%,  Test: 69.376%,  \n",
            "Epoch: 65, Loss: 0.975,  Train: 70.535%,  Valid: 69.791%,  Test: 68.864%,  \n",
            "Epoch: 66, Loss: 0.975,  Train: 70.522%,  Valid: 69.429%,  Test: 67.994%,  \n",
            "Epoch: 67, Loss: 0.974,  Train: 70.470%,  Valid: 69.580%,  Test: 68.251%,  \n",
            "Epoch: 68, Loss: 0.971,  Train: 70.630%,  Valid: 69.687%,  Test: 68.938%,  \n",
            "Epoch: 69, Loss: 0.968,  Train: 70.517%,  Valid: 69.657%,  Test: 68.592%,  \n",
            "Epoch: 70, Loss: 0.967,  Train: 70.804%,  Valid: 69.439%,  Test: 68.385%,  \n",
            "Epoch: 71, Loss: 0.964,  Train: 70.588%,  Valid: 69.173%,  Test: 67.646%,  \n",
            "Epoch: 72, Loss: 0.962,  Train: 70.504%,  Valid: 69.388%,  Test: 68.432%,  \n",
            "Epoch: 73, Loss: 0.961,  Train: 70.489%,  Valid: 69.405%,  Test: 68.247%,  \n",
            "Epoch: 74, Loss: 0.959,  Train: 70.509%,  Valid: 69.418%,  Test: 68.825%,  \n",
            "Epoch: 75, Loss: 0.957,  Train: 70.716%,  Valid: 69.680%,  Test: 69.134%,  \n",
            "Epoch: 76, Loss: 0.955,  Train: 70.968%,  Valid: 69.925%,  Test: 69.278%,  \n",
            "Epoch: 77, Loss: 0.955,  Train: 71.116%,  Valid: 70.194%,  Test: 69.463%,  \n",
            "Epoch: 78, Loss: 0.951,  Train: 71.105%,  Valid: 70.301%,  Test: 69.539%,  \n",
            "Epoch: 79, Loss: 0.950,  Train: 70.993%,  Valid: 70.224%,  Test: 69.788%,  \n",
            "Epoch: 80, Loss: 0.950,  Train: 70.875%,  Valid: 70.133%,  Test: 69.557%,  \n",
            "Epoch: 81, Loss: 0.943,  Train: 70.886%,  Valid: 70.137%,  Test: 69.251%,  \n",
            "Epoch: 82, Loss: 0.942,  Train: 70.875%,  Valid: 69.848%,  Test: 69.393%,  \n",
            "Epoch: 83, Loss: 0.943,  Train: 71.137%,  Valid: 69.982%,  Test: 68.726%,  \n",
            "Epoch: 84, Loss: 0.940,  Train: 71.034%,  Valid: 69.479%,  Test: 67.605%,  \n",
            "Epoch: 85, Loss: 0.936,  Train: 71.361%,  Valid: 69.858%,  Test: 68.734%,  \n",
            "Epoch: 86, Loss: 0.936,  Train: 71.249%,  Valid: 69.972%,  Test: 69.874%,  \n",
            "Epoch: 87, Loss: 0.936,  Train: 71.226%,  Valid: 69.942%,  Test: 69.804%,  \n",
            "Epoch: 88, Loss: 0.934,  Train: 71.423%,  Valid: 70.069%,  Test: 68.992%,  \n",
            "Epoch: 89, Loss: 0.931,  Train: 71.479%,  Valid: 69.036%,  Test: 67.261%,  \n",
            "Epoch: 90, Loss: 0.932,  Train: 71.359%,  Valid: 69.455%,  Test: 67.523%,  \n",
            "Epoch: 91, Loss: 0.931,  Train: 71.495%,  Valid: 70.130%,  Test: 68.955%,  \n",
            "Epoch: 92, Loss: 0.929,  Train: 71.524%,  Valid: 70.220%,  Test: 69.582%,  \n",
            "Epoch: 93, Loss: 0.925,  Train: 71.692%,  Valid: 70.016%,  Test: 68.294%,  \n",
            "Epoch: 94, Loss: 0.924,  Train: 71.789%,  Valid: 69.536%,  Test: 67.823%,  \n",
            "Epoch: 95, Loss: 0.921,  Train: 71.600%,  Valid: 69.828%,  Test: 68.510%,  \n",
            "Epoch: 96, Loss: 0.920,  Train: 71.783%,  Valid: 70.197%,  Test: 69.057%,  \n",
            "Epoch: 97, Loss: 0.920,  Train: 71.582%,  Valid: 70.398%,  Test: 69.095%,  \n",
            "Epoch: 98, Loss: 0.920,  Train: 71.718%,  Valid: 70.278%,  Test: 68.965%,  \n",
            "Epoch: 99, Loss: 0.917,  Train: 71.961%,  Valid: 70.361%,  Test: 69.566%,  \n",
            "Epoch: 100, Loss: 0.915,  Train: 71.789%,  Valid: 70.325%,  Test: 68.769%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7qxVZVdyYSX",
        "outputId": "b3ef633f-441d-4973-8010-af5849f5df5d"
      },
      "source": [
        "print_model_accuracy(best_gcn, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.67%, Valid: 70.27% Test: 69.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49hK34vUpSyN"
      },
      "source": [
        "## GraphSAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKsTeokbCcE"
      },
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout,\n",
        "                 normalize=False):\n",
        "      \n",
        "      super(SAGE, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [SAGEConv(input_dim, hidden_dim, normalize=normalize)] +\n",
        "          [SAGEConv(hidden_dim, hidden_dim, normalize=normalize) \n",
        "           for _ in range(num_layers - 2)]+\n",
        "          [SAGEConv(hidden_dim, output_dim, normalize=normalize)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLAwUs5vwrj3"
      },
      "source": [
        "### No normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIH9szzcuV7e",
        "outputId": "abbff3f9-624c-4618-a75a-0cd04801a4b3"
      },
      "source": [
        "sage = SAGE(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "            output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "            dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(sage.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sage, sage_best_acc = train_loop(sage.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.943,  Train: 31.739%,  Valid: 33.303%,  Test: 32.663%,  \n",
            "Epoch: 02, Loss: 2.582,  Train: 29.385%,  Valid: 33.649%,  Test: 32.541%,  \n",
            "Epoch: 03, Loss: 2.220,  Train: 35.980%,  Valid: 41.589%,  Test: 44.392%,  \n",
            "Epoch: 04, Loss: 2.047,  Train: 40.289%,  Valid: 44.793%,  Test: 46.409%,  \n",
            "Epoch: 05, Loss: 1.825,  Train: 44.133%,  Valid: 46.693%,  Test: 46.773%,  \n",
            "Epoch: 06, Loss: 1.688,  Train: 47.782%,  Valid: 50.203%,  Test: 50.022%,  \n",
            "Epoch: 07, Loss: 1.617,  Train: 49.836%,  Valid: 53.646%,  Test: 54.021%,  \n",
            "Epoch: 08, Loss: 1.539,  Train: 50.304%,  Valid: 54.066%,  Test: 54.256%,  \n",
            "Epoch: 09, Loss: 1.485,  Train: 51.090%,  Valid: 54.562%,  Test: 54.404%,  \n",
            "Epoch: 10, Loss: 1.448,  Train: 52.172%,  Valid: 54.938%,  Test: 54.799%,  \n",
            "Epoch: 11, Loss: 1.416,  Train: 53.626%,  Valid: 56.767%,  Test: 56.745%,  \n",
            "Epoch: 12, Loss: 1.384,  Train: 55.081%,  Valid: 58.190%,  Test: 58.361%,  \n",
            "Epoch: 13, Loss: 1.361,  Train: 56.581%,  Valid: 59.713%,  Test: 59.982%,  \n",
            "Epoch: 14, Loss: 1.346,  Train: 58.020%,  Valid: 60.740%,  Test: 61.373%,  \n",
            "Epoch: 15, Loss: 1.323,  Train: 58.915%,  Valid: 62.069%,  Test: 62.313%,  \n",
            "Epoch: 16, Loss: 1.300,  Train: 59.590%,  Valid: 62.143%,  Test: 62.556%,  \n",
            "Epoch: 17, Loss: 1.276,  Train: 60.202%,  Valid: 62.549%,  Test: 62.572%,  \n",
            "Epoch: 18, Loss: 1.258,  Train: 60.560%,  Valid: 62.831%,  Test: 62.932%,  \n",
            "Epoch: 19, Loss: 1.245,  Train: 61.054%,  Valid: 63.368%,  Test: 63.912%,  \n",
            "Epoch: 20, Loss: 1.232,  Train: 61.410%,  Valid: 64.049%,  Test: 64.381%,  \n",
            "Epoch: 21, Loss: 1.219,  Train: 62.468%,  Valid: 64.626%,  Test: 64.967%,  \n",
            "Epoch: 22, Loss: 1.209,  Train: 62.945%,  Valid: 64.942%,  Test: 65.317%,  \n",
            "Epoch: 23, Loss: 1.200,  Train: 63.550%,  Valid: 65.422%,  Test: 65.383%,  \n",
            "Epoch: 24, Loss: 1.191,  Train: 63.883%,  Valid: 65.502%,  Test: 65.552%,  \n",
            "Epoch: 25, Loss: 1.175,  Train: 64.255%,  Valid: 65.855%,  Test: 65.445%,  \n",
            "Epoch: 26, Loss: 1.162,  Train: 64.452%,  Valid: 65.771%,  Test: 65.370%,  \n",
            "Epoch: 27, Loss: 1.154,  Train: 64.675%,  Valid: 65.979%,  Test: 65.724%,  \n",
            "Epoch: 28, Loss: 1.147,  Train: 65.250%,  Valid: 66.207%,  Test: 66.132%,  \n",
            "Epoch: 29, Loss: 1.136,  Train: 65.650%,  Valid: 66.489%,  Test: 66.132%,  \n",
            "Epoch: 30, Loss: 1.132,  Train: 66.034%,  Valid: 66.761%,  Test: 66.187%,  \n",
            "Epoch: 31, Loss: 1.124,  Train: 66.202%,  Valid: 66.845%,  Test: 66.058%,  \n",
            "Epoch: 32, Loss: 1.116,  Train: 66.342%,  Valid: 66.777%,  Test: 66.056%,  \n",
            "Epoch: 33, Loss: 1.111,  Train: 66.473%,  Valid: 67.408%,  Test: 66.541%,  \n",
            "Epoch: 34, Loss: 1.102,  Train: 66.839%,  Valid: 67.539%,  Test: 67.064%,  \n",
            "Epoch: 35, Loss: 1.095,  Train: 67.032%,  Valid: 67.724%,  Test: 67.216%,  \n",
            "Epoch: 36, Loss: 1.090,  Train: 67.157%,  Valid: 67.905%,  Test: 67.422%,  \n",
            "Epoch: 37, Loss: 1.085,  Train: 67.511%,  Valid: 67.945%,  Test: 67.590%,  \n",
            "Epoch: 38, Loss: 1.081,  Train: 67.718%,  Valid: 68.160%,  Test: 67.321%,  \n",
            "Epoch: 39, Loss: 1.072,  Train: 67.700%,  Valid: 68.116%,  Test: 67.446%,  \n",
            "Epoch: 40, Loss: 1.069,  Train: 67.911%,  Valid: 68.274%,  Test: 67.630%,  \n",
            "Epoch: 41, Loss: 1.063,  Train: 67.902%,  Valid: 68.073%,  Test: 67.720%,  \n",
            "Epoch: 42, Loss: 1.059,  Train: 68.116%,  Valid: 68.234%,  Test: 67.971%,  \n",
            "Epoch: 43, Loss: 1.053,  Train: 68.239%,  Valid: 68.381%,  Test: 68.125%,  \n",
            "Epoch: 44, Loss: 1.050,  Train: 68.215%,  Valid: 68.697%,  Test: 68.370%,  \n",
            "Epoch: 45, Loss: 1.049,  Train: 68.451%,  Valid: 68.727%,  Test: 68.272%,  \n",
            "Epoch: 46, Loss: 1.041,  Train: 68.569%,  Valid: 68.586%,  Test: 67.839%,  \n",
            "Epoch: 47, Loss: 1.037,  Train: 68.789%,  Valid: 68.912%,  Test: 68.175%,  \n",
            "Epoch: 48, Loss: 1.031,  Train: 68.812%,  Valid: 68.630%,  Test: 68.562%,  \n",
            "Epoch: 49, Loss: 1.029,  Train: 68.977%,  Valid: 68.858%,  Test: 68.658%,  \n",
            "Epoch: 50, Loss: 1.024,  Train: 68.737%,  Valid: 69.006%,  Test: 68.479%,  \n",
            "Epoch: 51, Loss: 1.022,  Train: 68.970%,  Valid: 69.086%,  Test: 68.418%,  \n",
            "Epoch: 52, Loss: 1.017,  Train: 69.100%,  Valid: 68.942%,  Test: 68.294%,  \n",
            "Epoch: 53, Loss: 1.013,  Train: 69.251%,  Valid: 68.918%,  Test: 68.761%,  \n",
            "Epoch: 54, Loss: 1.012,  Train: 69.538%,  Valid: 69.137%,  Test: 68.685%,  \n",
            "Epoch: 55, Loss: 1.007,  Train: 69.155%,  Valid: 68.892%,  Test: 68.313%,  \n",
            "Epoch: 56, Loss: 1.002,  Train: 69.541%,  Valid: 69.016%,  Test: 68.683%,  \n",
            "Epoch: 57, Loss: 1.002,  Train: 69.527%,  Valid: 69.079%,  Test: 68.652%,  \n",
            "Epoch: 58, Loss: 0.998,  Train: 69.430%,  Valid: 68.720%,  Test: 68.498%,  \n",
            "Epoch: 59, Loss: 0.994,  Train: 69.799%,  Valid: 68.928%,  Test: 68.214%,  \n",
            "Epoch: 60, Loss: 0.993,  Train: 69.762%,  Valid: 68.912%,  Test: 68.451%,  \n",
            "Epoch: 61, Loss: 0.990,  Train: 69.855%,  Valid: 69.187%,  Test: 68.551%,  \n",
            "Epoch: 62, Loss: 0.987,  Train: 69.975%,  Valid: 69.002%,  Test: 68.852%,  \n",
            "Epoch: 63, Loss: 0.986,  Train: 69.775%,  Valid: 69.113%,  Test: 68.702%,  \n",
            "Epoch: 64, Loss: 0.978,  Train: 69.882%,  Valid: 69.106%,  Test: 69.062%,  \n",
            "Epoch: 65, Loss: 0.974,  Train: 69.956%,  Valid: 69.217%,  Test: 68.965%,  \n",
            "Epoch: 66, Loss: 0.974,  Train: 70.259%,  Valid: 69.472%,  Test: 68.874%,  \n",
            "Epoch: 67, Loss: 0.973,  Train: 70.339%,  Valid: 69.140%,  Test: 68.584%,  \n",
            "Epoch: 68, Loss: 0.967,  Train: 70.320%,  Valid: 69.291%,  Test: 68.881%,  \n",
            "Epoch: 69, Loss: 0.964,  Train: 69.994%,  Valid: 68.996%,  Test: 69.158%,  \n",
            "Epoch: 70, Loss: 0.965,  Train: 70.070%,  Valid: 68.939%,  Test: 68.920%,  \n",
            "Epoch: 71, Loss: 0.962,  Train: 70.363%,  Valid: 69.405%,  Test: 69.041%,  \n",
            "Epoch: 72, Loss: 0.958,  Train: 70.321%,  Valid: 69.126%,  Test: 68.953%,  \n",
            "Epoch: 73, Loss: 0.954,  Train: 70.326%,  Valid: 69.647%,  Test: 69.230%,  \n",
            "Epoch: 74, Loss: 0.954,  Train: 70.244%,  Valid: 69.204%,  Test: 69.140%,  \n",
            "Epoch: 75, Loss: 0.949,  Train: 70.145%,  Valid: 69.257%,  Test: 69.095%,  \n",
            "Epoch: 76, Loss: 0.947,  Train: 70.286%,  Valid: 69.113%,  Test: 69.109%,  \n",
            "Epoch: 77, Loss: 0.945,  Train: 70.395%,  Valid: 69.046%,  Test: 68.839%,  \n",
            "Epoch: 78, Loss: 0.943,  Train: 70.566%,  Valid: 69.247%,  Test: 69.045%,  \n",
            "Epoch: 79, Loss: 0.941,  Train: 70.505%,  Valid: 69.076%,  Test: 69.041%,  \n",
            "Epoch: 80, Loss: 0.939,  Train: 70.616%,  Valid: 69.217%,  Test: 69.261%,  \n",
            "Epoch: 81, Loss: 0.936,  Train: 70.662%,  Valid: 69.301%,  Test: 69.134%,  \n",
            "Epoch: 82, Loss: 0.935,  Train: 70.743%,  Valid: 69.126%,  Test: 69.064%,  \n",
            "Epoch: 83, Loss: 0.931,  Train: 70.685%,  Valid: 69.116%,  Test: 69.006%,  \n",
            "Epoch: 84, Loss: 0.928,  Train: 70.826%,  Valid: 68.992%,  Test: 69.267%,  \n",
            "Epoch: 85, Loss: 0.926,  Train: 70.756%,  Valid: 69.254%,  Test: 69.123%,  \n",
            "Epoch: 86, Loss: 0.929,  Train: 70.975%,  Valid: 69.583%,  Test: 69.222%,  \n",
            "Epoch: 87, Loss: 0.922,  Train: 70.977%,  Valid: 69.482%,  Test: 69.156%,  \n",
            "Epoch: 88, Loss: 0.920,  Train: 71.014%,  Valid: 69.509%,  Test: 69.175%,  \n",
            "Epoch: 89, Loss: 0.917,  Train: 71.173%,  Valid: 69.368%,  Test: 69.278%,  \n",
            "Epoch: 90, Loss: 0.917,  Train: 71.104%,  Valid: 69.690%,  Test: 69.290%,  \n",
            "Epoch: 91, Loss: 0.917,  Train: 71.203%,  Valid: 69.798%,  Test: 69.259%,  \n",
            "Epoch: 92, Loss: 0.913,  Train: 71.251%,  Valid: 69.630%,  Test: 69.329%,  \n",
            "Epoch: 93, Loss: 0.909,  Train: 71.424%,  Valid: 69.630%,  Test: 69.220%,  \n",
            "Epoch: 94, Loss: 0.908,  Train: 71.188%,  Valid: 69.439%,  Test: 69.020%,  \n",
            "Epoch: 95, Loss: 0.906,  Train: 71.179%,  Valid: 69.267%,  Test: 69.420%,  \n",
            "Epoch: 96, Loss: 0.902,  Train: 71.538%,  Valid: 69.965%,  Test: 69.492%,  \n",
            "Epoch: 97, Loss: 0.904,  Train: 71.648%,  Valid: 69.824%,  Test: 69.405%,  \n",
            "Epoch: 98, Loss: 0.899,  Train: 71.732%,  Valid: 70.143%,  Test: 69.313%,  \n",
            "Epoch: 99, Loss: 0.898,  Train: 71.830%,  Valid: 69.972%,  Test: 69.459%,  \n",
            "Epoch: 100, Loss: 0.897,  Train: 71.703%,  Valid: 69.915%,  Test: 69.592%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxMTkyK2y-Zb",
        "outputId": "863565f5-c089-40b7-b25a-548cd67a08e9"
      },
      "source": [
        "print_model_accuracy(best_sage, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.93%, Valid: 70.39% Test: 69.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksmxQNW_wuyv"
      },
      "source": [
        "### With normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUEmWo_KwfQB",
        "outputId": "fbab740a-307f-4b09-cf19-1cf778bfc168"
      },
      "source": [
        "sagenorm = SAGE(input_dim=args[\"input_dim\"], hidden_dim=args[\"hidden_dim\"], \n",
        "            output_dim=args[\"output_dim\"], num_layers=args[\"num_layers\"], \n",
        "            dropout=args[\"dropout\"], normalize=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(sagenorm.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_sagenorm, sagenorm_best_acc = train_loop(sagenorm.to(device), \n",
        "                                              data, optimizer, nll_loss, \n",
        "                                              args[\"epochs\"], split_idx, \n",
        "                                              dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.688,  Train: 24.851%,  Valid: 22.222%,  Test: 25.470%,  \n",
            "Epoch: 02, Loss: 3.432,  Train: 29.644%,  Valid: 32.340%,  Test: 36.185%,  \n",
            "Epoch: 03, Loss: 3.390,  Train: 30.794%,  Valid: 36.109%,  Test: 39.921%,  \n",
            "Epoch: 04, Loss: 3.353,  Train: 30.813%,  Valid: 37.807%,  Test: 41.164%,  \n",
            "Epoch: 05, Loss: 3.315,  Train: 31.129%,  Valid: 38.867%,  Test: 41.777%,  \n",
            "Epoch: 06, Loss: 3.282,  Train: 31.043%,  Valid: 38.521%,  Test: 40.856%,  \n",
            "Epoch: 07, Loss: 3.259,  Train: 30.323%,  Valid: 37.273%,  Test: 38.382%,  \n",
            "Epoch: 08, Loss: 3.243,  Train: 29.462%,  Valid: 35.719%,  Test: 36.385%,  \n",
            "Epoch: 09, Loss: 3.230,  Train: 28.435%,  Valid: 34.800%,  Test: 35.018%,  \n",
            "Epoch: 10, Loss: 3.216,  Train: 28.181%,  Valid: 35.233%,  Test: 35.586%,  \n",
            "Epoch: 11, Loss: 3.201,  Train: 28.875%,  Valid: 36.273%,  Test: 37.269%,  \n",
            "Epoch: 12, Loss: 3.187,  Train: 30.389%,  Valid: 39.001%,  Test: 40.759%,  \n",
            "Epoch: 13, Loss: 3.174,  Train: 32.539%,  Valid: 41.303%,  Test: 43.954%,  \n",
            "Epoch: 14, Loss: 3.161,  Train: 33.860%,  Valid: 42.387%,  Test: 45.865%,  \n",
            "Epoch: 15, Loss: 3.151,  Train: 34.388%,  Valid: 42.236%,  Test: 45.870%,  \n",
            "Epoch: 16, Loss: 3.143,  Train: 34.442%,  Valid: 41.511%,  Test: 45.326%,  \n",
            "Epoch: 17, Loss: 3.136,  Train: 33.794%,  Valid: 39.968%,  Test: 44.150%,  \n",
            "Epoch: 18, Loss: 3.129,  Train: 33.099%,  Valid: 38.548%,  Test: 43.380%,  \n",
            "Epoch: 19, Loss: 3.124,  Train: 33.512%,  Valid: 38.149%,  Test: 42.683%,  \n",
            "Epoch: 20, Loss: 3.119,  Train: 33.554%,  Valid: 38.186%,  Test: 42.518%,  \n",
            "Epoch: 21, Loss: 3.114,  Train: 34.138%,  Valid: 38.337%,  Test: 42.606%,  \n",
            "Epoch: 22, Loss: 3.110,  Train: 35.179%,  Valid: 39.709%,  Test: 43.713%,  \n",
            "Epoch: 23, Loss: 3.106,  Train: 36.291%,  Valid: 40.327%,  Test: 44.450%,  \n",
            "Epoch: 24, Loss: 3.102,  Train: 37.516%,  Valid: 41.679%,  Test: 45.863%,  \n",
            "Epoch: 25, Loss: 3.099,  Train: 39.015%,  Valid: 43.340%,  Test: 47.116%,  \n",
            "Epoch: 26, Loss: 3.095,  Train: 40.565%,  Valid: 45.565%,  Test: 48.664%,  \n",
            "Epoch: 27, Loss: 3.091,  Train: 42.282%,  Valid: 47.283%,  Test: 50.303%,  \n",
            "Epoch: 28, Loss: 3.088,  Train: 43.579%,  Valid: 49.109%,  Test: 51.847%,  \n",
            "Epoch: 29, Loss: 3.085,  Train: 45.167%,  Valid: 50.676%,  Test: 53.305%,  \n",
            "Epoch: 30, Loss: 3.081,  Train: 46.549%,  Valid: 51.767%,  Test: 54.019%,  \n",
            "Epoch: 31, Loss: 3.078,  Train: 47.411%,  Valid: 52.680%,  Test: 55.019%,  \n",
            "Epoch: 32, Loss: 3.075,  Train: 48.608%,  Valid: 53.371%,  Test: 55.239%,  \n",
            "Epoch: 33, Loss: 3.072,  Train: 49.814%,  Valid: 54.066%,  Test: 55.793%,  \n",
            "Epoch: 34, Loss: 3.069,  Train: 50.896%,  Valid: 54.821%,  Test: 56.420%,  \n",
            "Epoch: 35, Loss: 3.067,  Train: 52.150%,  Valid: 55.592%,  Test: 56.704%,  \n",
            "Epoch: 36, Loss: 3.064,  Train: 53.552%,  Valid: 56.572%,  Test: 57.379%,  \n",
            "Epoch: 37, Loss: 3.062,  Train: 54.490%,  Valid: 57.237%,  Test: 57.416%,  \n",
            "Epoch: 38, Loss: 3.060,  Train: 55.638%,  Valid: 57.878%,  Test: 57.961%,  \n",
            "Epoch: 39, Loss: 3.058,  Train: 56.055%,  Valid: 57.911%,  Test: 57.896%,  \n",
            "Epoch: 40, Loss: 3.056,  Train: 56.676%,  Valid: 58.210%,  Test: 57.916%,  \n",
            "Epoch: 41, Loss: 3.054,  Train: 57.199%,  Valid: 58.183%,  Test: 57.867%,  \n",
            "Epoch: 42, Loss: 3.053,  Train: 57.409%,  Valid: 58.425%,  Test: 57.809%,  \n",
            "Epoch: 43, Loss: 3.051,  Train: 57.524%,  Valid: 58.415%,  Test: 57.766%,  \n",
            "Epoch: 44, Loss: 3.049,  Train: 58.103%,  Valid: 58.817%,  Test: 58.081%,  \n",
            "Epoch: 45, Loss: 3.047,  Train: 58.691%,  Valid: 59.515%,  Test: 58.618%,  \n",
            "Epoch: 46, Loss: 3.045,  Train: 59.484%,  Valid: 60.106%,  Test: 59.396%,  \n",
            "Epoch: 47, Loss: 3.044,  Train: 60.219%,  Valid: 60.596%,  Test: 60.077%,  \n",
            "Epoch: 48, Loss: 3.042,  Train: 60.453%,  Valid: 60.985%,  Test: 60.352%,  \n",
            "Epoch: 49, Loss: 3.041,  Train: 60.880%,  Valid: 61.140%,  Test: 60.507%,  \n",
            "Epoch: 50, Loss: 3.039,  Train: 60.980%,  Valid: 61.502%,  Test: 60.589%,  \n",
            "Epoch: 51, Loss: 3.038,  Train: 61.333%,  Valid: 61.314%,  Test: 60.529%,  \n",
            "Epoch: 52, Loss: 3.035,  Train: 61.893%,  Valid: 61.918%,  Test: 61.280%,  \n",
            "Epoch: 53, Loss: 3.035,  Train: 62.501%,  Valid: 62.609%,  Test: 61.745%,  \n",
            "Epoch: 54, Loss: 3.033,  Train: 63.093%,  Valid: 63.277%,  Test: 62.632%,  \n",
            "Epoch: 55, Loss: 3.032,  Train: 63.549%,  Valid: 63.630%,  Test: 63.169%,  \n",
            "Epoch: 56, Loss: 3.031,  Train: 63.761%,  Valid: 63.878%,  Test: 63.570%,  \n",
            "Epoch: 57, Loss: 3.029,  Train: 64.210%,  Valid: 64.220%,  Test: 63.743%,  \n",
            "Epoch: 58, Loss: 3.028,  Train: 64.530%,  Valid: 64.355%,  Test: 63.938%,  \n",
            "Epoch: 59, Loss: 3.026,  Train: 64.767%,  Valid: 64.633%,  Test: 64.356%,  \n",
            "Epoch: 60, Loss: 3.025,  Train: 65.135%,  Valid: 65.395%,  Test: 64.517%,  \n",
            "Epoch: 61, Loss: 3.024,  Train: 65.354%,  Valid: 65.204%,  Test: 64.780%,  \n",
            "Epoch: 62, Loss: 3.022,  Train: 65.514%,  Valid: 65.452%,  Test: 64.885%,  \n",
            "Epoch: 63, Loss: 3.022,  Train: 65.743%,  Valid: 65.536%,  Test: 64.975%,  \n",
            "Epoch: 64, Loss: 3.020,  Train: 66.047%,  Valid: 65.844%,  Test: 65.224%,  \n",
            "Epoch: 65, Loss: 3.019,  Train: 66.416%,  Valid: 66.120%,  Test: 65.366%,  \n",
            "Epoch: 66, Loss: 3.018,  Train: 66.729%,  Valid: 66.455%,  Test: 65.473%,  \n",
            "Epoch: 67, Loss: 3.016,  Train: 66.747%,  Valid: 66.509%,  Test: 65.440%,  \n",
            "Epoch: 68, Loss: 3.016,  Train: 66.884%,  Valid: 66.858%,  Test: 65.603%,  \n",
            "Epoch: 69, Loss: 3.015,  Train: 67.064%,  Valid: 66.714%,  Test: 65.982%,  \n",
            "Epoch: 70, Loss: 3.013,  Train: 67.314%,  Valid: 66.660%,  Test: 65.860%,  \n",
            "Epoch: 71, Loss: 3.011,  Train: 67.580%,  Valid: 66.965%,  Test: 66.144%,  \n",
            "Epoch: 72, Loss: 3.011,  Train: 67.652%,  Valid: 67.274%,  Test: 66.393%,  \n",
            "Epoch: 73, Loss: 3.010,  Train: 67.751%,  Valid: 67.331%,  Test: 66.500%,  \n",
            "Epoch: 74, Loss: 3.008,  Train: 67.951%,  Valid: 67.412%,  Test: 66.687%,  \n",
            "Epoch: 75, Loss: 3.007,  Train: 68.389%,  Valid: 67.784%,  Test: 66.895%,  \n",
            "Epoch: 76, Loss: 3.006,  Train: 68.279%,  Valid: 67.747%,  Test: 66.932%,  \n",
            "Epoch: 77, Loss: 3.005,  Train: 68.397%,  Valid: 67.996%,  Test: 67.397%,  \n",
            "Epoch: 78, Loss: 3.004,  Train: 68.518%,  Valid: 68.009%,  Test: 67.235%,  \n",
            "Epoch: 79, Loss: 3.003,  Train: 68.739%,  Valid: 68.009%,  Test: 67.292%,  \n",
            "Epoch: 80, Loss: 3.002,  Train: 68.842%,  Valid: 68.197%,  Test: 67.681%,  \n",
            "Epoch: 81, Loss: 3.001,  Train: 68.888%,  Valid: 68.096%,  Test: 67.551%,  \n",
            "Epoch: 82, Loss: 3.000,  Train: 69.133%,  Valid: 68.338%,  Test: 67.671%,  \n",
            "Epoch: 83, Loss: 2.999,  Train: 69.120%,  Valid: 68.358%,  Test: 67.615%,  \n",
            "Epoch: 84, Loss: 2.998,  Train: 69.211%,  Valid: 68.123%,  Test: 67.648%,  \n",
            "Epoch: 85, Loss: 2.997,  Train: 69.271%,  Valid: 67.959%,  Test: 67.640%,  \n",
            "Epoch: 86, Loss: 2.996,  Train: 69.090%,  Valid: 68.341%,  Test: 67.648%,  \n",
            "Epoch: 87, Loss: 2.995,  Train: 69.248%,  Valid: 68.274%,  Test: 67.601%,  \n",
            "Epoch: 88, Loss: 2.993,  Train: 69.320%,  Valid: 68.418%,  Test: 67.662%,  \n",
            "Epoch: 89, Loss: 2.993,  Train: 69.525%,  Valid: 68.328%,  Test: 67.502%,  \n",
            "Epoch: 90, Loss: 2.992,  Train: 69.463%,  Valid: 68.462%,  Test: 67.609%,  \n",
            "Epoch: 91, Loss: 2.991,  Train: 69.511%,  Valid: 68.462%,  Test: 67.874%,  \n",
            "Epoch: 92, Loss: 2.990,  Train: 69.553%,  Valid: 68.241%,  Test: 67.821%,  \n",
            "Epoch: 93, Loss: 2.990,  Train: 69.655%,  Valid: 68.304%,  Test: 67.831%,  \n",
            "Epoch: 94, Loss: 2.989,  Train: 69.632%,  Valid: 68.462%,  Test: 68.027%,  \n",
            "Epoch: 95, Loss: 2.987,  Train: 69.684%,  Valid: 68.502%,  Test: 68.014%,  \n",
            "Epoch: 96, Loss: 2.986,  Train: 69.819%,  Valid: 68.663%,  Test: 67.901%,  \n",
            "Epoch: 97, Loss: 2.985,  Train: 69.996%,  Valid: 68.345%,  Test: 67.695%,  \n",
            "Epoch: 98, Loss: 2.985,  Train: 70.184%,  Valid: 68.422%,  Test: 67.702%,  \n",
            "Epoch: 99, Loss: 2.984,  Train: 70.235%,  Valid: 68.479%,  Test: 68.234%,  \n",
            "Epoch: 100, Loss: 2.983,  Train: 70.329%,  Valid: 68.717%,  Test: 68.292%,  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSIx_dZAzjLG",
        "outputId": "e1c1b6d1-9663-4169-e0c6-8f6335d71cd5"
      },
      "source": [
        "print_model_accuracy(best_sagenorm, data, split_idx, dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 71.93%, Valid: 70.13% Test: 68.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoETpGZQvPQ1"
      },
      "source": [
        "# GAT\n",
        "Scales badly need to decrease hidden_dim, otherwise cuda goes out of memory.\n",
        "See this: https://github.com/pyg-team/pytorch_geometric/issues/527"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd_zrnm3ujHe"
      },
      "source": [
        "class GAT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads,\n",
        "                 num_layers, dropout):\n",
        "      \n",
        "      super(GAT, self).__init__()\n",
        "\n",
        "      self.dropout = dropout\n",
        "      \n",
        "      self.convs = torch.nn.ModuleList(\n",
        "          [GATConv(input_dim, hidden_dim, heads, concat=False)] +\n",
        "          [GATConv(hidden_dim, hidden_dim, heads, concat=False) for _ in range(num_layers - 2)]+\n",
        "          [GATConv(hidden_dim, output_dim, heads, concat=False)]\n",
        "      )\n",
        "\n",
        "      self.bns = torch.nn.ModuleList(\n",
        "          [torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers - 1)]\n",
        "      )\n",
        "\n",
        "      self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "      for bn in self.bns:\n",
        "          bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "      out = x\n",
        "      for i in range(len(self.convs) - 1):\n",
        "        out = self.convs[i](out, adj_t)\n",
        "        out = self.bns[i](out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = torch.nn.functional.dropout(out, p=self.dropout)\n",
        "      \n",
        "      out = self.convs[-1](out, adj_t)\n",
        "      out = self.softmax(out)\n",
        "      return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "TLFdIsMwvpep",
        "outputId": "83d896a1-93d6-4aed-c4ad-654347dc1dc2"
      },
      "source": [
        "gat = GAT(input_dim=args[\"input_dim\"], hidden_dim=16,#args[\"hidden_dim\"], \n",
        "          output_dim=args[\"output_dim\"], heads=2,#args[\"heads\"],\n",
        "          num_layers=args[\"num_layers\"], dropout=args[\"dropout\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(gat.parameters(), lr=args[\"lr\"])\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "\n",
        "best_gat, gat_best_acc = train_loop(gat.to(device), \n",
        "                                      data, optimizer, nll_loss, \n",
        "                                      args[\"epochs\"], split_idx, \n",
        "                                      dataset_evaluator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8fa24b1ebbef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                       dataset_evaluator)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-2476ca512908>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, data, optimizer, loss_fn, epochs, split_idx, evaluator)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-48f1c4f14002>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, loss_fn, optimizer, train_idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-63c979b96503>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj_t)\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor, alpha: OptPairTensor, edge_attr: OptTensor)  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         out = self.propagate(edge_index, x=x, alpha=alpha, edge_attr=edge_attr,\n\u001b[0;32m--> 219\u001b[0;31m                              size=size)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                         \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_j, alpha_j, alpha_i, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m  \u001b[0;31m# Save for later use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_j\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 11.17 GiB total capacity; 10.48 GiB already allocated; 22.81 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4r5iZKav-A3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}